Введение

Набор программных инструментов, решающих различные задачи в области
автоматизированной обработки информации, вошёл в повседневный круг
обихода большого числа людей. К ним относятся различные локальные
приложения, такие как настольные базы данных или системы электронного
документооборота, а также распределённые продукты, требующие
значительно больших ресурсов, чем предоставляет персональный
компьютер, как, например, поисковая система в Интернет. При этом
интеллектуальная составляющая в применяемых алгоритмах остаётся всё
ещё достаточно слабой, всю работу, связанную с восприятием и
интерпретацией информации, продолжает выполнять человек.

Сфера искусственного интеллекта (ИИ), несмотря на то, что развивается
уже на протяжении нескольких десятилетий, не может похвастаться полным
достижением поставленных перед исследователями целей. Восприятие
информации и принятие решений в таком виде, как это делает человек,
остаётся вне достижимых научных пределов. Тем не менее, интенсивное
развитие получили некоторые отдельные отрасли, ранее считавшиеся
разделами ИИ, как, например, Data Mining, которой посвящена настоящая
курсовая работа.

Data Mining на русский язык можно перевести как "извлечение данных",
хотя, насколько можно судить, общепринятого перевода в русскоязычной
академической литературе пока нет. Далее в тексте будем использовать
английский вариант названия или заменять его сокращением DM. DM
охватывает подходы к обработке всех видов данных, включая графические
и звуковые. Мы же ограничимся более узкой сферой, посвящённой
обработке только текстовой информации, называемой Text Mining (TM).
TM прошёл несколько фаз своего развития [FIXME:здесь обязательно
ссылка], о которых будет сказано ниже. Наиболее перспективный подход в
настоящее время связан с попыткой разработки методов, обладающих
способностью выделять понятия или, как иногда говорят, сущности,
обозначаемые словами в тексте. Машина при этом всё равно не способна
понять смысла текста, но можно попытаться определить множество
сущностей, их атрибуты и связи между ними. Понятия для компьютера
остаются "безликими", это не более чем отдельные переменные в
программном коде, но этого может быть достаточно, чтобы получить
удобные инструменты обработки информации нового
поколения. Актуальность и перспективность такого подхода подтверждает
пристальный интерес к этой технологии со стороны крупных корпораций,
таких как Google [FIXME:здесь тоже обязательно ссылка].

Построение множества понятий, упоминаемых в тексте, значительно
затруднено большим количеством имён собственных различных персоналий,
местностей и пр. Легко понятные каждому человеку имена и названия,
такие как Москва, Ньютон, Платон, для компьютера остаются
недоступными. Каких-либо методов связать их с понятиями "человек" или
"город", кроме как иметь в своём распоряжении заранее готовую базу
данных (БД), видимо, практически нет, по крайней мере, в настоящую
эпоху. По этой причине весьма интересный потенциал представляет собой
свободная энциклопедия "Википедия"...

\section{Обзор алгоритмов DM}

В процессе своей эволюции DM прошёл несколько этапов развития. Ранние
подходы представляли собой различного рода эвристические алгоритмы.
Их суть сводилась к вычислению некоторой оценки, получаемой
при~обработке текста, на основе которой принималось решение. К ним
относятся алгоритмы классификации и кластеризации, кратко описанные
ниже.Более развитые алгоритмы, составляющие группу извлечения
информации, позволяют прослеживать различные логические взаимосвязи и
выделять сущности на основе содержания текста. Их отличительная
особенность в том, что в них присутствует в той или иной форме понятие
"объекта", с которым связано множество атрибутов и их значений.

\subsection{Классификация}

%%ХЗдесь необходимо указать, что поступает на вход алгоритма, т.е. что
%%  анализируется множество документов.

Классификация является одной из самых простых и распространенных задач
DM. В результате классификации получаем набор данных, помеченных
принадлежностями к одному из заданных классов.  На начальном этапе
задается тренировочный набор данных, для элементов которого уже
%% "модель классификации" - неоппределённое понятие. Надо либо расписать, либо заменить на что-нибудь более правильное.
определена принадлежность к классу. Теперь выбирается модель 
классификации. Точность ее работы определяется следующим образом:
тестовый набор данных классифицируется, а результат сравнивается с
имеющимся изначально. Если модель показывает приемлемые результаты, то
с ее помощью выполняется классификация остальных данных.  Виды(?)

%%Обязательно один вариант привести подробно и ещё парочку бегло, если попадутся.

\subsection{Кластеризация}

%%Традиционный комментарий --- что на входе, т.е. опять сказать, что обрабатываем массив документов.

Кластеризация является задачей, сходной с классификацией, но более сложной. В данном случае
классы заранее не определены, и суть состоит в разделении исходных данных по группам на основании
%%Тут попадается слово "схожесть", но это не академический термин. Лучше брать слово "близость".
их близости. Причем из одного кластера объекты должны быть более близкими, а из разных --- менее.

%% Опять же один вариант кластеризации подробно, ещё парочку --- бегло.

\subsection{Извлечение информации}

Подходы на основе классификации и кластеризации не представляют сейчас
большого интереса, поскольку уже достаточно хорошо изучены и обладают
относительно слабым ппотенциалом для дальнейшего развития. Более
перспективными в этой ситуации являются задачи, связанные с
извлечением информации.

Задачу извлечения информации можно рассматривать, как часть общей
задачи понимания естественных языков. Только в данном случае вдобавок
известно, какой именно тип семантической информации
извлекается. Основной целью является выделить части текста и
обозначить атрибуты для них. Вышесказанное можно проиллюстрировать на
примере.  [пояснять ли, почему на английском?] Дано предложение на
естественном языке: "Robert L. James, chairman and chief executive
officer of McCann-Erickson, is going to retire on July 1st.  He will
be replaced by John J.Donner, Jr., the agencies chief operating
officer."  Получим следующие атрибуты: организация (McCann-Erickson),
должность (chief executive officer), дата(July 1), имя уходящего с
должности человека (Robert L. James), and имя приходящего на должность
человека (John J. Donner, Jr.).

Задачу извлечения информации естественным образом можно
декомпозировать: в результате она представляет собой серию
последовательных шагов, таких, как обработка данных, обычно включающая
разделение на токены, сегментация предложений, определение частей речи
и идентификация имен собственных (людей, мест, организаций).

%%Валентина, этого примера недостаточно.Тут должно быть ясное описание, что такое извлечение информации, плавно сводящееся к графу из последней книги. 
%% Можно дословно перевести какой-нибудь подходящий фрагмент из первой статьи.

В данной работе речь пойдет об англоязычной версии энциклопедии «Википедия».
Такой выбор обусловлен тем, что эта версия является самой крупной,
а так же большинство подходов TM ориентировано на работу с данными
%% Мы сильно привязаны к библиотекам обработки естественного языка на англ. языке (CoreNLP).
%% В большинстве случаев можно ссылаться просто на это.
на английском языке.(?) Один из возможных вариантов работы представлен 
в статье Wikipedia mining for an association web thesaurus construction [7]. 

Как база (corpus) для извлечения знаний, "Википедия" обладает 
такими важными характеристиками, как большое число статей, регулярные обновления
(live updates), большое количество ссылок между статьями (a dense link structure),
короткий текст ссылки и идентификация каждого понятия по URL. Учитывая эти особенности,
авторы статьи ставят себе цель построить словарь, отражающий близость 
между понятиями - тезаурус. Подобные попытки уже предпринимались, но, по словам
авторов, ни в одной из них не обращалась должного внимания на улучшение 
точности и масштабируемости. Так что их задачей стало то же построение тезауруса,
но уже со стремлением улучшить вышеописанные характеристики.

В качестве входного материала используется множество статей "Википедия", причем статьи 
рассматриваются как понятия.
%% При чтении тут сразу появляется неясность. Почему мы это получаем? У нас сама Википедия - это массив статей со ссылками. 
%% Тут надо написать точнее, в чём разница.
В результате работы получаем словарь (тезаурус), состоящий из множества статей (понятий) и 
гиперссылок между ними.
Учитывая написанное выше, тезаурус может быть представлен в виде графа G = {V,E}
(V : множество статей, E: множество ссылок).
В большей степени, связность (близость) между двумя статьями(понятиями), vi и vj,
зависит от двух параметров:
- количество путей между vi и vj;
- длина каждого пути между vi и vj;
Путь между двумя статьями – это последовательность ссылок (количество таких ссылок
– длина пути), которая соединяет данные статьи. Очевидно, что таких путей может 
быть несколько. 

Близость тем больше, чем больше количество путей между соответствующими
статьями. Также, если статьи в графе расположены рядом и содержат гиперссылки на
одни и те же статьи, то их близость будет больше, чем у более далеко расположенных
статей.

Определенную роль играет количество входящих ссылок. Рассмотрим случай, когда существует 
некоторая статья, на которую ссылается достаточно большое число других. Таким образом
данная статья имеет большое количество "коротких" путей, что в свою очередь означает,
если опираться только на два приведенных выше параметра, что эта она тесно связана
с ссылающимися на нее статьями, т.е. понятия соответствующие этим статьям являются достаточно
близкими. Однако, если обратить внимание на число в ходящих ссылок (в данном случае
оно велико), то понятия, соответствующие подобным статьям, можно обозначить как общеупотребительные,
и в большинстве случаев они не очень значимы. 

Говоря о сложности нахождения значения приведенных выше параметров, стоит заметить, что
задача подсчета количества входящих ссылок является достаточно очевидно решаемой, но для 
подсчета длин путей между статьями придется прибегнуть к нестандартным подходам. 
Для подсчета количества путей используется степень матрицы смежности графа, 
построенного на основании наличия ссылки между двумя соответствующими статьями. 
Степень - максимальная рассматриваемая длина пути. В результате возведения матрицы
в степень в каждом i,j-элементе мы получаем суммарное количество путей между i-ой
и j-ой статьей, длина которых не больше максимальной. В связи с тем, что размерность
такой матрицы велика, но она при этом разрежена, для ее хранения и выполнения операций 
умножения используются бинарные деревья: для каждой строки - свое дерево.

Как правило, тезаурус включает в себя несколько типов семантик, из которых %%Кажется, слово "семантик" здесь не совсем верно. Не уверен, но как-то странно читается.
две являются базовыми: синонимы и антонимы. Предположительно, синонимы можно найти из 
связей между статьями, но вопрос нахождения антонимов в статье не рассматривается, что 
является существенным недостатком. 

/subsection{WEKA}

По причине недостатков подхода, описанного выше, было принято решение перейти к 
изучению более эффективных и перспективных методов ТМ. Внимание на себя обратил ныне активно
развивающийся проект Weka, о котором упоминалось в статьях(?) Google [ссылка].
Weka представляет собой свободно распространяемый пакет для анализа данных, 
написанный на языке Java в университете Уайкато (Новая Зеландия). Weka позволяет 
выполнять такие задачи анализа данных, как классификация, кластеризация, извлечение 
правил ассоциаций и выборка атрибутов. 


Классификация и кластеризация не представляют интереса в силу своей изученности в отличие от
последних двух, которые в соответствии с общей классификацией методов ТМ можно отнести к задачам
извлечения информации.

Алгоритмы извлечения правил ассоциаций и выборки атрибутов являются развитыми алгоритмами,
для них данные об объектах хранятся в виде фиксированного списка атрибутов. Множество объектов
составляет базу данных (БД). Приведём пример подобной базы данных:
•	множество объектов - множество видов животных;
•	вариант множества атрибутов: вид (str),средний вес (float), кормление молоком (bool),
наличие шерсти (bool), длина названия вида (int).

Для "Википедия" подобная база данных может выглядеть следующим образом: в качестве множества
объектов рассматривается множество статей, а атрибутами являются, к примеру, 1000 самых часто
встречаемых и информативных понятий(?). 

Извлечение правил ассоциаций заключается в поиске интересных и полезных закономерностей в БД.
Сказанное выше стоит проиллюстрировать на примере (БД описана выше). Выберем два атрибута: средний 
вес и наличие шерсти, можем получить следующее логическое выражение: если средний вес равен 
50 кг, и есть шерсть, то вид - волк или шимпанзе, и т. д.

Выборка атрибутов - задача сортировки атрибутов объектов по их информационной содержательности.
Проиллюстрируем данную задачу на примере, используя БД, описанную выше. В БД после работы 
алгоритма выборки атрибутов должны получить список: вид, средний вес, кормление молоком, 
наличие шерсти. Здесь нет атрибута “длина названия вида”, т.к. он, очевидно, не несет
никакой информационной ценности.

Задача выборки атрибутов и извлечения правил ассоциаций являются интересными и перспективными
для изучения. [переформулировать?]

В результате более детальной работы с Weka выяснилось, что подход для решения интересующих задач
достаточно поверхностный и не рассчитан на работу с большим количеством атрибутов. Алгоритмы, 
представленные в Weka, - эвристические, т. е. даже если бы они были приспособлены для работы с 
большим количеством атрибутов, то в результате позволили ли бы лишь приблизиться к цели, а не
достичь ее.
