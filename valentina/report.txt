
Data Mining на русский язык можно перевести как "извлечение данных",
хотя, насколько можно судить, общепринятого перевода в русскоязычной
академической литературе пока нет. Далее в тексте будем использовать
английский вариант названия или заменять его сокращением DM. DM
охватывает подходы к обработке всех видов данных, включая графические
и звуковые. Мы же ограничимся более узкой сферой, посвящённой
обработке только текстовой информации, называемой Text Mining (TM).
TM прошёл несколько фаз своего развития [FIXME:здесь обязательно
ссылка], о которых будет сказано ниже. Наиболее перспективный подход в
настоящее время связан с попыткой разработки методов, обладающих
способностью выделять понятия или, как иногда говорят, сущности,
обозначаемые словами в тексте. Машина при этом всё равно не способна
понять смысла текста, но можно попытаться определить множество
сущностей, их атрибуты и связи между ними. Понятия для компьютера
остаются "безликими", это не более чем отдельные переменные в
программном коде, но этого может быть достаточно, чтобы получить
удобные инструменты обработки информации нового
поколения. Актуальность и перспективность такого подхода подтверждает
пристальный интерес к этой технологии со стороны крупных корпораций,
таких как Google [FIXME:здесь тоже обязательно ссылка].

Построение множества понятий, упоминаемых в тексте, значительно
затруднено большим количеством имён собственных различных персоналий,
местностей и пр. Легко понятные каждому человеку имена и названия,
такие как Москва, Ньютон, Платон, для компьютера остаются
недоступными. Каких-либо методов связать их с понятиями "человек" или
"город", кроме как иметь в своём распоряжении заранее готовую базу
данных (БД), видимо, практически нет, по крайней мере, в настоящую
эпоху. По этой причине весьма интересный потенциал представляет собой
свободная энциклопедия "Википедия"...

\section{Обзор алгоритмов DM}

В процессе своей эволюции DM прошёл несколько этапов развития. Ранние
подходы представляли собой различного рода эвристические алгоритмы.
Их суть сводилась к вычислению некоторой оценки, получаемой
при~обработке текста, на основе которой принималось решение. К ним
относятся алгоритмы классификации и кластеризации, кратко описанные
ниже. Более развитые алгоритмы, составляющие группу извлечения
информации, позволяют прослеживать различные логические взаимосвязи и
выделять сущности на основе содержания текста. Их отличительная
особенность в том, что в них присутствует в той или иной форме понятие
"объекта", с которым связано множество атрибутов и их значений.

\subsection{Классификация}

Классификация является одной из самых простых и распространенных задач
DM. Анализируется множество документов (D) и в результате каждому документу
ставится в соответствие один из классов(L).  На начальном этапе
задается обучающий набор данных, для элементов которого уже
%% "модель классификации" - неопределённое понятие. Надо либо расписать, либо заменить на что-нибудь более правильное.
определена принадлежность к классу. Теперь выбирается алгоритм (метод?) 
классификации. Точность его работы определяется следующим образом:
обучающий набор данных классифицируется, а результат сравнивается с
имеющимся изначально. Если алгоритм показывает приемлемые результаты, то
с ее помощью выполняется классификация остальных данных.  

Для большей наглядности разберем некоторые из методов. 

Метод ближайших соседей.
Данный алгоритм является одним из самых хорошо изученных и эффективных среди
алгоритмов классификации текстов.
Для определения класса обрабатываемого документа предлагается использовать 
его схожесть с документами из обучающего набора. Таким образом, класс обрабатываемого
документа может быть получен на основании принадлежности к классам похожих документов
из обучающего набора. Данный подход так же известен, как "Метод k-ближайших соседей", 
если для рассмотрения выбирается k документов.

Существует множество различных способов подсчета близости между двумя документами.
Один из самых примитивных заключается в подсчете числа common words в каждом документе.
%% не уверена насчет значения этого словосочетания: то ли общеупотребимые слова, то ли 
%%просто общие у двух документов.
Очевидно, что результат нужно нормализовать, так как тексты могут быть разной длины.
Но в этом случае необходимо учитывать, что одни и те же слова могут иметь множество
различных значений, что приводит к варьированию информационного содержимого.
Другой стандартный способ основан на применении косинусной меры близости. Для этого пространство 
документов преобразуется в векторную модель: каждый документ представляется в виде
m-мерного вектора (m - размер обучающего набора), то есть документ d можно представить
 в виде численного вектора признаков w(d) = (x(d,t1),..., x(d,tm)), причем каждому элементу
вектора соответствуют слова (группа слов). Таким образом, размер вектора определяется количеством 
лов (групп слов).

Для определения степени значимости слова в выбранном документе из заданной коллекции
используется такое понятие, как вес. Большой вес соответствует словам, часто встречающимся 
в нужных документах, но редко во всей коллекции. Таким образом, вес w(d,t) для слова t
из документа d рассчитываются, как произведение собственной частотности слова (term frequency) %%Стоит ли пояснять tf?
и инвертированного значения частоты, с которой это слово встречается в документе.
Чтобы полученное произведение не зависело от длины документов, необходима нормализация.
[здесь будет формула]
Вес равен нулю, если данное слово не представлено в документе.
Из сказанного выше можно заключить, что документ d может быть представлен в виде вектора,
состоящего из весов слов, а близость двух документов d1 и d2 определяется, как следующее 
скалярное произведение векторов:
[тоже формула]

Для того, чтобы определить, принадлежит ли документ di некоторому классу Lm, близость 
S(di,dj) для  всех документов dj из обучающего набора должна быть определена. Затем
выбирается k документов, наиболее близких ("соседей") к заданному d. Количество "представителей"
класса среди соседей можно использовать как вероятность принадлежности данного документа
к этому классу: документу определяют принадлежность к классу, наиболее широко представленному
среди соседей. 

При выборе числа соседей k необходимо учитывать, что при k = 1 алгоритм неустойчив к шумовым
выбросам: он даёт ошибочные классификации не только на самих объектах-выбросах, но и на 
ближайших к ним объектах других классов. Но при k=m алгоритм чрезмерно устойчив и вырождается 
в константу. Таким образом, крайние значения  нежелательны. На практике оптимальное значение
параметра  определяют по критерию скользящего контроля, чаще всего — методом исключения объектов
по одному (leave-one-out cross-validation).
[использован www.machinelearning.ru ]

Деревья принятия решений.
Деревья принятия решений - стандартный инструмент для решения задач классификации в ДМ. 
Деревья решений представляют собой классификатор, состоящий из набора правил, применяемых
последовательно. Лучше всего проиллюстрировать принцип работы деревьев решений в ходе процесса
обучения, на начальном этапе которого работа производится с полным обучающим набором.
Данный алгоритм использует стратегию "разделяй и властвуй": выбирается некоторое слово ti из
обучающего набора М, которое наиболее точно может указывать на принадлежность
документа к классу. На следующем шаге обучающий набор M разделяется на два подмножества:
подмножество Мi+ , состоящее из документов, содержащих выбранное слово ti, и Mi-, состоящее из
документов, не содержащих данное слово. Данная процедура рекурсивно повторяется для Mi+ и Mi-
до тех пор, пока не получится подмножество, состоящее из документов одного класса. В результате
получаем дерево [эквивалент tree of rules ?], в листьях которого записаны actual classes.

\subsection{Кластеризация}


Кластеризация является задачей, сходной с классификацией, но более сложной. На вход так же поступает массив
документов (D), только в данном случае классы заранее не определены, и суть состоит в разделении исходных
данных по группам на основании их близости. Причем из одного кластера объекты должны быть более близкими,
а из разных --- менее.

Одним из наиболее часто используемых алгоритмов кластеризации является алгоритм
к- средних (k-means). Его распространенность обусловлена простотой реализации,
возможностью работать с большими объемами информации, так же алгоритм показывает
хорошие результаты именно применительно к кластеризации текстов. 

Основная идея алгоритма.
На первом шаге определяется число кластеров k. Затем случайно выбирается k документов - начальных
центров кластеров, иначе центроидов (центроид – это вектор, элементы которого представляют собой 
средние значения признаков, вычисленные по всем записям кластера). Документы распределяются по 
кластерам в зависимости от близости к тому или иному центроиду. После чего пересчитываются центроиды 
для каждого кластера. 

Процесс продолжается дот тех пор, пока значения центроидов изменяются. Так же можно задать число 
итераций, по достижении  которого работа алгоритма завершается.

Помимо этого "классического" алгоритма существует ряд других, которые можно назвать альтернативными.

К примеру, алгоритм "совместной кластеризации" (co-clustering). Он представляет собой совершенно
иной подход к кластеризации, заключающийся в синхронной кластеризации документов и слов.

Идея другого алгоритма, называющегося "алгоритм нечеткой кластеризации", состоит в том, что в результате
получается не четкое разделение по кластерам, как в случае классических алгоритмов, а набор "степеней принадлежности"
документа к тому или иному кластеру.

Хорошим примером использования некоторых из вышеприведенным идей описана в статье
Wikipedia mining for an association web thesaurus construction. Авторы предлагают
алгоритм построения тезауруса - словаря, отражающего близость между словами.
В качестве корпуса для извлечения знаний авторы используют энциклопедию "Википедия"
в связи с тем, что она обладает рядом таких важных характеристик, как большое число 
статей, регулярные обновления (live updates), большое количество ссылок между статьями 
(a dense link structure), короткий текст ссылки и идентификация каждого понятия по URL.
Подобные попытки уже предпринимались, но, по словам авторов, ни в одной из них не обращалась
должного внимания на улучшение точности и масштабируемости. Так что их задачей стало то же 
построение тезауруса, но уже со стремлением улучшить вышеописанные характеристики.


В качестве входного материала используется множество статей "Википедия", причем статьи 
рассматриваются как понятия. Такое возможно, так как каждая статья отражает объекты реального
мира и различные понятия.
В результате работы получаем словарь (тезаурус), состоящий из множества статей (понятий) и 
гиперссылок между ними.
Учитывая написанное выше, тезаурус может быть представлен в виде графа G = {V,E}
(V : множество статей, E: множество ссылок).
В большей степени, связность (близость) между двумя статьями (понятиями), vi и vj,
зависит от двух параметров:
- количество путей между vi и vj;
- длина каждого пути между vi и vj;
Путь между двумя статьями – это последовательность ссылок (количество таких ссылок
– длина пути), которая соединяет данные статьи. Очевидно, что таких путей может 
быть несколько. 

Близость тем больше, чем больше количество путей между соответствующими
статьями. Также, если статьи в графе расположены рядом и содержат гиперссылки на
одни и те же статьи, то их близость будет больше, чем у более далеко расположенных
статей.

Определенную роль играет количество входящих ссылок. Рассмотрим случай, когда существует 
некоторая статья, на которую ссылается достаточно большое число других. Таким образом
данная статья имеет большое количество "коротких" путей, что в свою очередь означает,
если опираться только на два приведенных выше параметра, что эта она тесно связана
со ссылающимися на нее статьями, т.е. понятия соответствующие этим статьям являются достаточно
близкими. Однако если обратить внимание на число входящих ссылок (в данном случае
оно велико), то понятия, соответствующие подобным статьям, можно обозначить как общеупотребительные,
и в большинстве случаев они не очень значимы. 

Говоря о сложности нахождения значения приведенных выше параметров, стоит заметить, что
задача подсчета количества входящих ссылок является достаточно очевидно решаемой, но для 
подсчета длин путей между статьями придется прибегнуть к нестандартным подходам. 
Для подсчета количества путей используется степень матрицы смежности графа, 
построенного на основании наличия ссылки между двумя соответствующими статьями. 
Степень - максимальная рассматриваемая длина пути. В результате возведения матрицы
в степень в каждом i,j-элементе мы получаем суммарное количество путей между i-ой
и j-ой статьей,  длина которых не больше максимальной. 
На словах данные действия кажутся достаточно простыми, но при реализации могут возникнуть 
сложности. Использовать матрицу смежности и стандартную оперцию умножения не стоит, потому что,
к примеру, "Википедия" содержит около 1.3 миллиона статей, таким образом только для хранения
данных может понадобиться несколько терабайт. Помимо этого, для выполнения операций умножения
понадобится невероятное количество времени, так как трудоемкость порядка O(N^3).
Однако, матрица смежности разрежена: большая часть ее элементов - нули, таким образом,
для того, чтобы обеспечить масштабируемость, необходимо использовать специальные "сжимающие"
структуры данных и различные методы анализа данных. В связи с этим авторы предлагают использовать
такую структуру данных, как "двойное бинарное дерево" (Dual binary tree) и алгоритм умножения, 
учитывающий особенности подобной структуры.
Для достижения эффекта сжатия в бинарных деревьях хранятся только ненулевые элементы матрицы 
смежности. Двойное бинарное дерево состоит из двух типов бинарных деревьев: i-дерево и j-дерево.
Каждый элемент i-дерева соответствует строке матрицы смежности и содержит ссылку на корень j-дерева.
Следовательно, двойное бинарное дерево представляет собой N+1 бинарное дерево. В результате 
использования подобной структуры запись и чтение данных производится очень быстро, так как
трудоемкость составляет всего O(logN) в обоих случаях.
Next, we define the multiplication algorithm for the DBT as follows:
Алгоритм умножения для подобной структуры представлен ниже:

1 for i belongs i-Tree
2   for j belongs j-Tree(i)
3     for k belongs j-Tree(j)
4       Ri,k := Ri,k + aj,k • ai,j ;


Функция j-Tree(i) извлекает все элементы из i-ой строки матрицы смежности A.
aj,k - элемент j-ой строки и k-го столбца матрицы. Количество итерацией первого
цикла будет равно N, для последующих циклов это число будет стремиться к среднему 
количеству ссылок в статье - M. Таким образом, общее число шагов будет равно O(M2NLonN).
[22:42:50] Денис Жарков: Число M находится в интервале от 20 до 40 в случае статьи "Википедия", 
что значительно меньше размера исходной матрицы N. В итоге, мы получаем результат, 
сохраненный в другом двойном бинарном дереве R.

Одним из основных минусов описанного выше алгоритма построения тезауруса можно считать тот факт,
что ничего не говорится о нахождении антонимов, хотя они являются неотъемлемой частью подобного словаря.
Что касается синонимов, то их, предположительно можно найти из связей между статьями.


\subsection{Извлечение информации}

Подходы на основе классификации и кластеризации не представляют сейчас
большого интереса, поскольку уже достаточно хорошо изучены и обладают
относительно слабым потенциалом для дальнейшего развития. Более
перспективными в этой ситуации являются задачи, связанные с
извлечением информации.

Тексты на естественном языке содержат значительное количество информации,
которая напрямую не поддается автоматической обработке. Однако, можно использовать
возможности компьютера для работы с большими массивами текстов и извлечения
необходимой информации из отдельных слов, предложений или частей текста.
Таким образом задача извлечения информации заключается в получении структурированной
информации из неструктурированного текста на естественном языке и может рассматриваться
как отдельная часть общей задачи понимания естественных языков. 

Задача извлечения информации включает в себя следующие подзадачи:
1. Распознавание именованных сущностей.
   Эта подзадача в свою очередь разделяется еще на несколько более мелких.
   
   Непосредственно извлечение именованных сущностей, к примеру, названий географических
   объектов, людей, организаций и прочего.
   
   Разрешение анафор и кореференций(Coreference), то есть обнаружение связей между уже
   извлеченными именами и названиями. К примеру, "Iternational Business Machines"
   и "IBM" соответствуют одной и той же реалии. Или в случае двух предложений
   "George Harrison wrote a new song. He was excited of it." необходимо, чтобы
   местоимение "he" ассоциировалось с прежде полученным "George Harrison".
   
   Распознавание отношений между словами в предложении. К примеру, из предложения
   "Mick Jagger plays in the Rolling Stones" нужно выделить следующее отношение:
   ЧЕЛОВЕК играет в ГРУППЕ.
   
2. Извлечение информации из слабоструктурированных текстов. 
   Примером этой задачи может служить нахождение таблиц в документе и последующая
   работа с ними.
   
3. Нахождение терминологии, специфичной для данного документа.

Одним из возможных способов представления текста для машинной обработки является
представление его в виде объектов с атрибутами, т.е. извлекаются части текста и 
и для них обозначаются атрибуты.

Вышесказанное можно проиллюстрировать на примере. Дано предложение на 
естественном языке: "Robert L. James, chairman and chief executive
officer of McCann-Erickson, is going to retire on July 1st.  He will
be replaced by John J.Donner, Jr., the agencies chief operating
officer."  Получим следующие атрибуты: организация (McCann-Erickson),
должность (chief executive officer), дата(July 1), имя уходящего с
должности человека (Robert L. James), имя приходящего на должность
человека (John J. Donner, Jr.).

При реализации этого подхода можно столкнуться с некоторыми сложностями. 
К примеру, при составлении базы данных, элементами которой буду объекты и
атрибуты, придется выбирать фиксированное число атрибутов для представления
текста, что тоже представляется достаточно сложной задачей.

Другим интересным способом представление текста в формате, пригодном для
автоматической обработки, является использование так называемого графа знаний - 
специфического графа, отражающего связь между словами и группами слов в предложении.

/subsection{WEKA}

По причине недостатков подхода, основанного на построении тезауруса, было принято решение перейти к 
изучению более эффективных и перспективных методов ТМ. Внимание на себя обратил ныне активно
развивающийся проект Weka, о котором упоминалось в статьях(?) Google [ссылка].
Weka представляет собой свободно распространяемый пакет для анализа данных, 
написанный на языке Java в университете Уайкато (Новая Зеландия). Weka позволяет 
выполнять такие задачи анализа данных, как классификация, кластеризация, извлечение 
правил ассоциаций и выборка атрибутов. 


Классификация и кластеризация не представляют интереса в силу своей изученности в отличие от
последних двух, которые в соответствии с общей классификацией методов ТМ можно отнести к задачам
извлечения информации.

Алгоритмы извлечения правил ассоциаций и выборки атрибутов являются развитыми алгоритмами,
для них данные об объектах хранятся в виде фиксированного списка атрибутов. Множество объектов
составляет базу данных (БД). Приведём пример подобной базы данных:
•	множество объектов - множество видов животных;
•	вариант множества атрибутов: вид (str),средний вес (float), кормление молоком (bool),
наличие шерсти (bool), длина названия вида (int).

Для "Википедия" подобная база данных может выглядеть следующим образом: в качестве множества
объектов рассматривается множество статей, а атрибутами являются, к примеру, 1000 самых часто
встречаемых и информативных понятий(?). 

Извлечение правил ассоциаций заключается в поиске интересных и полезных закономерностей в БД.
Сказанное выше стоит проиллюстрировать на примере (БД описана выше). Выберем два атрибута: средний 
вес и наличие шерсти, можем получить следующее логическое выражение: если средний вес равен 
50 кг, и есть шерсть, то вид - волк или шимпанзе, и т. д.

Выборка атрибутов - задача сортировки атрибутов объектов по их информационной содержательности.
Проиллюстрируем данную задачу на примере, используя БД, описанную выше. В БД после работы 
алгоритма выборки атрибутов должны получить список: вид, средний вес, кормление молоком, 
наличие шерсти. Здесь нет атрибута “длина названия вида”, т.к. он, очевидно, не несет
никакой информационной ценности.

Задача выборки атрибутов и извлечения правил ассоциаций являются интересными и перспективными
для изучения. [переформулировать?]

В результате более детальной работы с Weka выяснилось, что подход для решения интересующих задач
достаточно поверхностный и не рассчитан на работу с большим количеством атрибутов. Алгоритмы, 
представленные в Weka, - эвристические, т. е. даже если бы они были приспособлены для работы с 
большим количеством атрибутов, то в результате позволили ли бы лишь приблизиться к цели, а не
достичь ее.
