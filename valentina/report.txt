Введение

Набор программных инструментов, решающих различные задачи в области
автоматизированной обработки информации, вошёл в повседневный круг
обихода большого числа людей. К ним относятся различные локальные
приложения, такие как настольные базы данных или системы электронного
документооборота, а также распределённые продукты, требующие
значительно больших ресурсов, чем предоставляет персональный
компьютер, как, например, поисковая система в Интернет. При этом
интеллектуальная составляющая в применяемых алгоритмах остаётся всё
ещё достаточно слабой, всю работу, связанную с восприятием и
интерпретацией информации, продолжает выполнять человек.

Сфера искусственного интеллекта (ИИ), несмотря на то, что развивается
уже на протяжении нескольких десятилетий, не может похвастаться полным
достижением поставленных перед исследователями целей. Восприятие
информации и принятие решений в таком виде, как это делает человек,
остаётся вне достижимых научных пределов. Тем не менее, интенсивное
развитие получили некоторые отдельные отрасли, ранее считавшиеся
разделами ИИ, как, например, Data Mining, которой посвящена настоящая
курсовая работа.

Data Mining на русский язык можно перевести как "извлечение данных",
хотя, насколько можно судить, общепринятого перевода в русскоязычной
академической литературе пока нет. Далее в тексте будем использовать
английский вариант названия или заменять его сокращением DM. DM
охватывает подходы к обработке всех видов данных, включая графические
и звуковые. Мы же ограничимся более узкой сферой, посвящённой
обработке только текстовой информации, называемой Text Mining (TM).
TM прошёл несколько фаз своего развития [FIXME:здесь обязательно
ссылка], о которых будет сказано ниже. Наиболее перспективный подход в
настоящее время связан с попыткой разработки методов, обладающих
способностью выделять понятия или, как иногда говорят, сущности,
обозначаемые словами в тексте. Машина при этом всё равно не способна
понять смысла текста, но можно попытаться определить множество
сущностей, их атрибуты и связи между ними. Понятия для компьютера
остаются "безликими", это не более чем отдельные переменные в
программном коде, но этого может быть достаточно, чтобы получить
удобные инструменты обработки информации нового
поколения. Актуальность и перспективность такого подхода подтверждает
пристальный интерес к этой технологии со стороны крупных корпораций,
таких как Google [FIXME:здесь тоже обязательно ссылка].

Построение множества понятий, упоминаемых в тексте, значительно
затруднено большим количеством имён собственных различных персоналий,
местностей и пр. Легко понятные каждому человеку имена и названия,
такие как Москва, Ньютон, Платон, для компьютера остаются
недоступными. Каких-либо методов связать их с понятиями "человек" или
"город", кроме как иметь в своём распоряжении заранее готовую базу
данных (БД), видимо, практически нет, по крайней мере, в настоящую
эпоху. По этой причине весьма интересный потенциал представляет собой
свободная энциклопедия "Википедия"...

В данной работе была предпринята попытка использования методов Data Mining для работы (?) с электронной англоязычной энциклопедией Википедия.
(Написать, почему Википедия)
  Data mining - процесс выделения полезных закономерностей  (useful patterns) в больших массивах данных.
Data Mining тесно связан с такими областями науки, как искусственный интеллект и математическая статистика.
Когда в качестве данных выступает текст, то можно говорить о такой разновидности  Data Mining, как Text Mining.
Text mining - процесс выделения полезных закономерностей (useful patterns) в больших массивах текста на естественном языке. Text mining = Text Data Mining.

Существуют следующие основные направления Data mining:
1. классификация;
2. кластеризация;
3. извлечение правил ассоциаций;
4. выборка атрибутов.
Приведённые направления носят в основном эвристическую природу.
Классификация
d -> l, D contains d, P contains p.
где D - исходное множество документов, L – множество классов документов. В отличие от кластеризации, множество классов задано изначально.
Предполагается наличие обучающего набора образцов документов, для которого доступна информация о принадлежности к классам.
Кластеризация 
d -> p, P contains p, D contains d
где D - исходное множество документов, P – множество выделенных кластеров.
Документы разделяются на кластеры таким образом, что:
•	внутри одного кластера документы однородны;
•	 документы из разных кластеров разнородны.
Объекты и атрибуты 
Для развитых алгоритмов данные об объекте хранятся в виде фиксированного списка атрибутов. Множество объектов составляет базу данных (БД). Приведём пример подобной базы данных:
•	множество объектов _ множество видов животных;
•	вариант множества атрибутов: вид (str),средний вес (float), кормление молоком (bool),наличие шерсти (bool), длина названия вида (int).
Извлечение правил ассоциаций
Извлечение правил ассоциаций заключается в поиске интересных и полезных закономерностей в БД.
Пример: выберем два атрибута: средний вес и наличие шерсти, можем получить следующее логическое выражение: если средний вес равен 50 кг, и есть шерсть, то вид - волк или шимпанзе, и т. д.
Выборка атрибутов
Выборка атрибутов - задача сортировки атрибутов объектов по их информационной содержательности.
Пример: в БД после работы алгоритма выборки атрибутов должны получить список: вид, средний вес, кормление молоком, наличие шерсти. Здесь нет атрибута “длина названия
вида”, т.к. он, очевидно, не несет никакой информационной ценности.
(Рассказать про изученность первых двух направлений. Рассказать, что заниматься буду последними двумя)
Основная часть.
Один из возможных способов работы с Википедией  представлен в статье Wikipedia mining for an association web thesaurus construction [7]. Авторы предлагают алгоритм создания тезауруса (словаря, отражающего «похожесть» между словами), в основе которого лежит использование таких показателей, как количество путей между двумя статьями, длины этих путей, а так же количество входящих ссылок отдельно взятой статьи. Последний показатель указывает, является ли текущая статья general term. Если величина этого показателя сравнительно велика, то перед нами general term.
Для подсчета количества путей используется степень матрицы смежности графа, построенного на основании наличия ссылки между двумя соответствующими статьями. Так как размерность такой матрицы велика, но сама матрица при этом разрежена, для ее хранения и выполнение операции умножения используется n бинарных деревьев (свое для каждой строки).
Как правило, тезаурус включает в себя несколько типов семантик, из которых две являются базовыми: синонимы и антонимы. Предположительно, синонимы можно найти, используя связи между статьями, но проблема нахождения антонимов в статье не обсуждается. Без идеи о нахождении антонимов этот подход не является полезным. (переформулировать?)
Проект Weka.
Weka - это свободно распространяемый пакет для анализа данных, написанный на языке Java в университете Уайкато (Новая Зеландия).
Weka позволяет выполнять такие важные задачи анализа данных, как извлечение правил ассоциаций и выборка атрибутов. В качестве источника данных могут использоваться реляционные БД или структурированные данные в формате CSV.
(рассказать про упоминание в Google, ссылка)
В ходе более подробного изучения выяснилось, что алгоритмы Weka не пригодны для наших целей: они не рассчитаны на работу с большим количеством атрибутов. Помимо этого, алгоритмы являются эвристическими, так что всерьез на них рассчитывать не стоит.


 
