\section {Задача построения базы данных на основе Wikipedia}

Data Mining на русский язык можно перевести как ``извлечение данных'',
хотя, насколько можно судить, общепринятого перевода в русскоязычной
академической литературе пока нет. Далее в тексте будем использовать
английский вариант названия или заменять его сокращением DM. 
DM охватывает подходы к обработке всех видов данных, включая графические и звуковые. 
Мы же ограничимся более узкой сферой, посвящённой
обработке только текстовой информации, называемой Text Mining (TM).
TM в~своём развитии прошёл несколько фаз \cite{text_mining_survey}, о~которых будет сказано ниже. 
Наиболее перспективный современный подход связан с попыткой разработки методов, обладающих
способностью выделять понятия или, как иногда говорят, сущности, обозначаемые словами в тексте. 
Машина при этом всё равно не способна 
понять смысл текста, но можно попытаться определить множество
сущностей, их атрибуты и связи между ними. Понятия для компьютера
остаются "безликими", это не более чем отдельные переменные в
программном коде, но этого может быть достаточно, чтобы получить
удобные инструменты обработки информации нового
поколения. Актуальность и перспективность такого подхода подтверждает
пристальный интерес к этой технологии со стороны крупных корпораций,
таких как Google \cite{fixme}.

Построение множества понятий, упоминаемых в тексте, значительно
затруднено большим количеством имён собственных различных персоналий,
местностей и пр. Легко понятные каждому человеку имена и названия,
такие как Москва, Ньютон, Платон, для компьютера остаются
недоступными. Каких-либо методов связать их с понятиями ``человек'' или
``город'', кроме как иметь в своём распоряжении заранее готовую базу
данных (БД), видимо, практически нет, по крайней мере, в настоящую эпоху. 
По этой причине весьма интересный потенциал представляет собой
свободная энциклопедия Wikipedia.

%%АFIXME:Здесь немного выходит неоконченная мысль, надо попытаться свести к каким-нибудь практическим примерам.

\section{Обзор алгоритмов Data Mining}

В процессе своей эволюции DM прошёл несколько этапов развития.
 Ранние методы в~основном представляли собой различного рода эвристические алгоритмы.
Их суть сводилась к вычислению некоторой оценки, получаемой
при~обработке текста, на основе которой принималось решение. К ним
относятся алгоритмы классификации и кластеризации, кратко описанные ниже. 
Более развитые алгоритмы, составляющие группу методов извлечени информации, 
позволяют прослеживать различные логические взаимосвязи и выделять сущности на основе содержания текста. 
Их отличительная особенность в том, что в них присутствует в той или иной форме понятие
``объекта'', с которым связано множество атрибутов и их значений.

\subsection{Классификация}

Классификация является одной из самых простых и распространенных задач DM. 
Анализируется множество документов $D$ и в результате каждому документу
ставится в соответствие один из классов$L$.  На начальном этапе
задается обучающий набор данных, для элементов которого уже
определена принадлежность к классу. 
Затем выбирается алгоритм классификации. 
Точность его работы определяется следующим образом:
обучающий набор данных классифицируется, а результат сравнивается с заданным заранее эталоном.
Если результат работы алгоритма проходит некоторый необходимый уровень совпадений с~эталоном, 
то с его помощью выполняется классификация остальных данных.  
Для~примера рассмотрим некоторые конкретные варианты классификации.

\subsubsection{Метод ближайших соседей}

Данный алгоритм является одним из наиболее детально изученных и эффективных среди
алгоритмов классификации текстов.
Для определения класса обрабатываемого документа предлагается использовать 
его близость с документами из обучающего набора. Таким образом, класс обрабатываемого
документа может быть получен на основании принадлежности к классам близких документов
из обучающего набора. 
Данный подход также известен, как ``Метод $k$-ближайших соседей'', 
если для рассмотрения выбирается $k$ документов.

Существует множество различных способов подсчета близости между двумя документами.
Один из самых примитивных заключается в подсчете числа общих слов в каждом документе.
Очевидно, что результат нужно нормализовать, так как тексты могут быть разной длины.
Но в этом случае необходимо учитывать, что одни и те же слова могут иметь множество
различных значений, что приводит к варьированию информационного содержимого.

Другой стандартный способ основан на применении косинусной меры близости. Для этого пространство 
документов преобразуется в векторную модель. Прежде всего стоит ввести наиболее часто используемые
обозначения: $D$ - набор документов, $T = {t_1, \dots, t_m}$ - словарь, т.е. набор всех различных слов,
встречающихся в D. Абсолютная частота слова $t \in T$ в документе $d \in D$ определяется величной
$tf(d,t)$, называемой так же собственной частотой слова.  
Тогда каждый документ представляется в виде $m$-мерного вектора (m - количество слов в словаре $T$),
т.~е. документ $d$ можно представить в виде числового вектора признаков: 

$$w(d) = (x(d,t_1),\dots, x(d,t_m)),$$ 

причем каждому элементу вектора $x(d, t_i)$ соответствуют слова (группа слов) $t_i$, где $i = 1..m $.

Для определения степени значимости слова в выбранном документе из заданной коллекции
используется такое понятие, как ``вес''. 
Большой вес соответствует словам, часто встречающихся 
в нужных документах, но редко во всей коллекции. 
Таким образом, вес $w(d,t)$ для слова $t$
из документа $d$ расчитывается как произведение собственной частоты употребления слова
и инвертированного значения частоты, которое определяется как $$idf(t) = \log (N/n_t)$$,
где $N$- число документов в наборе, а $n_t$ - число документов, содержащих слово $t$.

Чтобы полученное произведение не зависело от длины документов, необходима нормализация:
  

Вес должен быть равен нулю, если данное слово не встречается в документе.

Определив понятие веса, можем представить документ $d$ как вектор весов слов:
$$w(d) = (w(d,t_1), \dots, w(d,t_m))$$

Если документ $d$ может быть представлен в~виде вектора, приведенного выше, 
то близость двух документов $d_1$ и $d_2$ определяется, как следующее скалярное произведение векторов:

$$dist(d_1,d_2) = \sqrt{\sum \limits_{k=1}^{m}(|w(d_1,t_k)-w(d_2,t_k)|)^2}$$

Для~определения принадлежности документа $d_i$ некоторому классу $L_m$ 
должна быть определена близость $S(d_i,d_j)$ для  всех документов $d$ из~обучающего набора, 
которая определяется как:  
$$S(d_i,d_j)=\sum \limits_{k=1}^{m}(w(d_i,t_k)w(d_j,t_k))$$

Затем выбирается $k$ документов, наиболее близких  к заданному $d$, назовём их соседями. 
Количество экземпляров класса среди соседей можно использовать как вероятность принадлежности данного документа к этому классу: 
документу определяют принадлежность к классу, наиболее широко представленному среди соседей. 

При выборе числа соседей $k$ необходимо учитывать, что при $k = 1$ алгоритм неустойчив к шумовым
выбросам: он даёт ошибочные классификации не только на самих объектах-выбросах, но и на 
ближайших к ним объектах других классов. 
Но при $k = m$ алгоритм чрезмерно устойчив и вырождается 
в константу. Таким образом, крайние значения  нежелательны. На практике оптимальное значение
параметра  определяют по критерию скользящего контроля, чаще всего — методом исключения объектов
по одному (leave-one-out cross-validation) \cite{machinelearning} 

\subsubsection{Деревья принятия решений}

Деревья принятия решений --- стандартный инструмент для решения задач классификации в DM. 

Деревья решений представляют собой классификатор, состоящий из набора правил, применяемых последовательно. 
Лучше всего проиллюстрировать принцип работы деревьев решений в ходе процесса
обучения, на начальном этапе которого работа производится с полным обучающим набором.
Данный алгоритм использует стратегию ``разделяй и властвуй'': 
выбирается некоторое слово $t_i$ из обучающего набора $М$, 
которое наиболее точно может указывать на принадлежность документа к классу. 
На следующем шаге обучающий набор $M$ разделяется на два подмножества:
подмножество $М_{i+}$ , состоящее из документов, содержащих выбранное слово $t_i$, и $M_{i-}$, 
состоящее из документов, не содержащих данное слово. Данная процедура рекурсивно повторяется для $M_{i+}$ и $M_{i-}$
до тех пор, пока не получится подмножество, состоящее из документов одного класса. В результате
получаем дерево разбиения, в~листьях которого записаны требуемые классы.

\subsection{Кластеризация}

Кластеризация является задачей, сходной с классификацией, но более сложной. 
На вход так же поступает массив 
документов $D$, но в данном случае классы заранее не определены, и суть состоит в разделении исходных
данных по группам на основании их близости. 
Причем из одного кластера объекты должны быть более близкими,
а из разных --- менее.

\subsubsection{Алгоритм $k$-средних}

Одним из наиболее часто используемых алгоритмов кластеризации является алгоритм $k$-средних ($k$-means). 
Его распространенность обусловлена простотой реализации и возможностью работать с большими объемами информации.
Помимо этого  алгоритм показывает
хорошие результаты именно применительно к кластеризации текстов. 

В~ходе работы алгоритма на первом шаге определяется число кластеров $k$. 
Затем случайно выбирается $k$ документов --- начальных центров кластеров, иначе центроидов 
(центроид --- это вектор, элементы которого представляют собой средние значения признаков, вычисленные по всем записям кластера). 
Документы распределяются по кластерам в зависимости от близости к тому или иному центроиду. 
Затем центроиды пересчитываются для каждого кластера. 
Процесс продолжается до тех пор, пока значения центроидов изменяются. 
Так же можно задать число итераций, по достижении  которого работа алгоритма завершается.

\subsubsection{Альтернативные подходы к кластеризации}

Помимо этого классического алгоритма, существует ряд других, которые можно назвать альтернативными.
К примеру, алгоритм ``совместной кластеризации'' (co-clustering). 
Он представляет собой совершенно
иной подход к кластеризации, заключающийся в синхронной кластеризации документов и слов.

Идея другого алгоритма, называемого ``алгоритм нечеткой кластеризации'', состоит в том, что в результате
генерируется не четкое разделение по кластерам, как в случае классических алгоритмов, а набор степеней принадлежности
документа к тому или иному кластеру.

\subsection{Построение тезауруса для Wikipedia}

Предпринимались попытки применения описанных идей к~обработке материалов Wikipedia.
Интересный пример можно найти в~статье \cite{thesaurus}.
её авторы предлагают алгоритм построения тезауруса. В общем смысле тезурус ---словарь, %% --- словаря, отражающего близость между словами.
отражающий семантические отношения между словами, в данном случае  - близость между 
словами.                                     
В качестве корпуса для извлечения знаний авторы используют множество статей Wikipedia,
принимая во внимание такие сильные её стороны, как большое число 
статей, регулярные обновления (live updates), большое количество ссылок между статьями 
(a dense link structure), короткий текст ссылки и идентификация каждого понятия по URL.
Подобные попытки уже предпринимались, но, по словам авторов, ни в одной из них не обращалась
должного внимания на улучшение точности и масштабируемости. Так что их задачей стало то же 
построение тезауруса, но уже со стремлением улучшить вышеописанные характеристики.

В качестве входного материала используется множество статей, причем статьи рассматриваются как понятия. 
Такое возможно, поскольку каждая статья отражает объекты реального мира и различные понятия.

В результате получаем значение функции, аргументами которой являются две статьи, соотвествующие понятиями,
степень схожести между которыми необходимо узнать.

Так как Викикпедия состоит из множества статей (понятий) и гиперссылок между ними, то может быть представлена
в виде графа $G = \{V,E\}$, где 
$V$  --- это множество статей
а $E$ ---это множество ссылок.

Близость двух статей (понятий) $v_i$ и $v_j$  в~сильной степени зависит от двух параметров:

\begin{itemize}

\item{
количество путей между $v_i$ и $v_j$;
}

\item {
длины каждого пути между $v_i$ и $v_j$.
}

\end{itemize}

Путь между двумя статьями --- это последовательность ссылок 
(количество таких ссылок определяет длину пути),
которая соединяет данные статьи. 
Очевидно, что таких путей может быть несколько. 

Близость тем больше, чем больше количество путей между соответствующими статьями. 
Аналогично, если статьи в графе расположены рядом и содержат гиперссылки на
одни и те же статьи, то их близость будет больше, чем у более  отдалённых статей.

Важную роль играет количество входящих ссылок. 
Рассмотрим случай, когда существует некоторая статья, на которую ссылается большое число других статей. 
Такая статья имеет большое количество ``коротких'' путей, что в свою очередь означает,
что она тесно связана со~ссылающимися на нее статьями, 
т.~е. понятия, соответствующие этим статьям, являются достаточно близкими. 
Однако если обратить внимание на число входящих ссылок (в данном случае оно велико), 
то понятия, соответствующие подобным статьям, можно обозначить как общеупотребительные,
и в большинстве случаев они не очень значимы. 

Говоря о сложности нахождения значения приведенных выше параметров, стоит заметить, что
задача подсчета количества входящих ссылок является тривиально решаемой, но для 
подсчета длин путей между статьями придется прибегнуть к нестандартным подходам. 
Для подсчета количества путей используется степень матрицы смежности графа, 
построенного на основании наличия ссылки между двумя соответствующими статьями. 
Степень --- это максимальная рассматриваемая длина пути. В результате возведения матрицы
%%FIXME:заменила ребро на элемент с индексами i,j. мне кажется, так достаточно понятно
в степень в каждом элементе с индексами $i$, $j$ мы получаем суммарное количество путей между статьёй $v_i$  и статьёй $v_j$,
причем длина каждого из таких путей не больше максимальной. 

На словах данные действия кажутся достаточно простыми, но при реализации могут возникнуть сложности. 
Использовать матрицу смежности и стандартную операцию умножения не стоит, потому что,
к примеру, Wikipedia содержит около 1.3 миллиона статей, 
следовательно,  только для хранения данных может понадобиться несколько терабайт дискового пространства. 
Помимо этого, для выполнения операций умножения
понадобится чрезмерно много времени, т.~к. трудоемкость операции составляет $O(n^3)$.
Тем не менее, поскольку матрица смежности разрежена, и большая часть ее элементов --- нули, 
для обеспечения масштабируемости можно использовать специальные сжимающие структуры данных и различные методы анализа данных. 
В связи с этим авторы предлагают использовать такую структуру данных, как двойное бинарное дерево (Dual binary tree) и алгоритм умножения, 
учитывающий особенности подобной структуры.
Для достижения эффекта сжатия в бинарных деревьях хранятся только ненулевые элементы матрицы 
смежности. Двойное бинарное дерево состоит из двух типов бинарных деревьев: 
$i$-дерево и $j$-дерево.
Каждый элемент $i$-дерева соответствует строке матрицы смежности и содержит ссылку на корень $j$-дерева.
Следовательно, двойное бинарное дерево представляет собой $n+1$ бинарное дерево: одно $i$-дерева и N  $j$-деревьев. 
 %%FIXME: В статье есть картинка, иллюстрирующая это дерево. Стоит ее сюда вставить?
В результате использования подобной структуры запись и чтение данных производится очень быстро, так как
трудоемкость составляет всего $O(\log n)$ в обоих случаях.
%%FIXME:Next, we define the multiplication algorithm for the DBT as follows:
Алгоритм умножения для подобной структуры представлен ниже:

%%1 for i belongs i-Tree
%%2   for j belongs j-Tree(i)
%%3     for k belongs j-Tree(j)
%%4       Ri,k := Ri,k + aj,k • ai,j ;


Функция $JTree(i)$ извлекает все элементы из $i$-ой строки матрицы смежности $A$.
$A_{jk}$ --- это элемент $j$-ой строки и $k$-го столбца матрицы. 
Количество итерацией первого цикла равняется $n$, для последующих циклов это число будет стремиться к среднему 
количеству ссылок в статье --- $M$. 
Таким образом, общее число шагов будет равно $O(M^2 n \log n)$. %%FIXME:Валентина, напишите пожалуйста формулу с пробелами между обозначениями, чтобы правильно вклинить в tex.

Число $M$ находится в интервале от 20 до 40 в случае статей ``Википедия'', %%FIXME:Тут имеется в виду какая-то конкретная статья?
что значительно меньше размера исходной матрицы $n$. В итоге, мы получаем результат, 
сохраненный в другом двойном бинарном дереве $R$.

Одним из основных минусов описанного выше алгоритма построения тезауруса можно считать тот факт,
что ничего не говорится о нахождении антонимов, хотя они являются неотъемлемой частью подобного словаря.
Что касается синонимов, то их нетрудно можно найти из связей между статьями, введя желаемый порог близости.

\subsection{Извлечение информации}

Подходы на основе классификации и кластеризации не представляют сейчас
большого интереса, поскольку уже достаточно хорошо изучены и обладают
относительно слабым потенциалом для дальнейшего развития. 
Более перспективными в этой ситуации являются задачи, связанные с
извлечением информации.
Тексты на естественном языке содержат значительное количество информации,
которая напрямую не поддается автоматической обработке. Однако, можно использовать
возможности компьютера для работы с большими массивами текстов и извлечения
необходимой информации из отдельных слов, предложений или частей текста.
Таким образом задача извлечения информации заключается в получении структурированной
информации из неструктурированного текста на естественном языке и может рассматриваться
как отдельная часть общей задачи понимания естественных языков. 

Задача извлечения информации включает в себя следующие подзадачи:
1. Распознавание именованных сущностей.
   Эта подзадача в свою очередь разделяется еще на несколько более мелких.
   
   Непосредственно извлечение именованных сущностей, к примеру, названий географических
   объектов, людей, организаций и прочего.
   
   Разрешение анафор и кореференций(Coreference), то есть обнаружение связей между уже
   извлеченными именами и названиями. К примеру, "Iternational Business Machines"
   и "IBM" соответствуют одной и той же реалии. Или в случае двух предложений
   "George Harrison wrote a new song. He was excited of it." необходимо, чтобы
   местоимение "he" ассоциировалось с прежде полученным "George Harrison".
   
   Распознавание отношений между словами в предложении. К примеру, из предложения
   "Mick Jagger plays in the Rolling Stones" нужно выделить следующее отношение:
   ЧЕЛОВЕК играет в ГРУППЕ.
   
2. Извлечение информации из слабоструктурированных текстов. 
   Примером этой задачи может служить нахождение таблиц в документе и последующая
   работа с ними.
   
3. Нахождение терминологии, специфичной для данного документа.

Одним из возможных способов представления текста для машинной обработки является
представление его в виде объектов с атрибутами, т.е. извлекаются части текста и 
и для них обозначаются атрибуты.

Вышесказанное можно проиллюстрировать на примере. Дано предложение на 
естественном языке: "Robert L. James, chairman and chief executive
officer of McCann-Erickson, is going to retire on July 1st.  He will
be replaced by John J.Donner, Jr., the agencies chief operating
officer."  Получим следующие атрибуты: организация (McCann-Erickson),
должность (chief executive officer), дата(July 1), имя уходящего с
должности человека (Robert L. James), имя приходящего на должность
человека (John J. Donner, Jr.).

При реализации этого подхода можно столкнуться с некоторыми сложностями. 
К примеру, при составлении базы данных, элементами которой буду объекты и
атрибуты, придется выбирать фиксированное число атрибутов для представления
текста, что тоже представляется достаточно сложной задачей.

Другим интересным способом представление текста в формате, пригодном для
автоматической обработки, является использование так называемого графа знаний - 
специфического графа, отражающего связь между словами и группами слов в предложении.

/subsection{WEKA}

По причине недостатков подхода, основанного на построении тезауруса, было принято решение перейти к 
изучению более эффективных и перспективных методов ТМ. Внимание на себя обратил ныне активно
развивающийся проект Weka \cite{weka}, о котором упоминалось в статьях Google [ссылка]. %% я подобную ссылку не нашла
Weka представляет собой свободно распространяемый пакет для анализа данных, 
написанный на языке Java в университете Уайкато (Новая Зеландия). Weka позволяет 
выполнять такие задачи анализа данных, как классификация, кластеризация, извлечение 
правил ассоциаций и выборка атрибутов, помимо этого включате в себя полный набор
средств по предобработке данных. Таким образом, предварительную обработку данных 
и сравнение различных алгоритмов моно производиться достаточно просто, так как 
требуется всего лишь один формат исходных данных.



Классификация и кластеризация не представляют интереса в силу своей изученности в отличие от
последних двух, которые в соответствии с общей классификацией методов ТМ можно отнести к задачам
извлечения информации.

Алгоритмы извлечения правил ассоциаций и выборки атрибутов являются развитыми алгоритмами,
для них данные об объектах хранятся в виде фиксированного списка атрибутов. Множество объектов
составляет базу данных (БД). Приведём пример подобной базы данных:
•	множество объектов - множество видов животных;
•	вариант множества атрибутов: вид (str),средний вес (float), кормление молоком (bool),
наличие шерсти (bool), длина названия вида (int).

Для Википедия подобная база данных может выглядеть следующим образом: в качестве множества
объектов рассматривается множество статей, а атрибутами являются, к примеру, 1000 самых часто
встречаемых и информативных понятий(?). 

Извлечение правил ассоциаций заключается в поиске интересных и полезных закономерностей в БД.
Сказанное выше стоит проиллюстрировать на примере (БД описана выше). Выберем два атрибута: средний 
вес и наличие шерсти, можем получить следующее логическое выражение: если средний вес равен 
50 кг, и есть шерсть, то вид - волк или шимпанзе, и т. д.

Выборка атрибутов - задача сортировки атрибутов объектов по их информационной содержательности.
Проиллюстрируем данную задачу на примере, используя БД, описанную выше. В БД после работы 
алгоритма выборки атрибутов должны получить список: вид, средний вес, кормление молоком, 
наличие шерсти. Здесь нет атрибута “длина названия вида”, т.к. он, очевидно, не несет
никакой информационной ценности.

На первый взгляд описанные выше подходы показались интересными ддя изучения и дальнейшего
применения для Википедия. Но при более детальной работе с Weka выяснилось, что
построение БД с объектами и атрибутами для Википедия является нетривиально решаемой задачей:
как минимум, необходимо составить конечный список атрибутов, причем он должен иметь смысл 
в контексте каждого объекта БД. Очевидно, что таких атрибутов будет достаточно много,
на что алгоритмы Weka не рассчитаны. Так же на данный момент неясно, как эти проблемы
могут быть решены.
 
Описанные ранее подходы, основанные на классификации и кластеризации, тоже не лишены недостатков.
Сами по себе алгоритмы классификации и кластеризации являюся эвристическими, из чего следует, что
в результате мы лишь приближаемся к поставленной цели, а так как они достаточно хорошо изучены, то 
каких-либо серьезных изменений ожидать не стоит.

\subsection {Граф Знаний}
 
Ни один из рассмотренных выше методов не оправдал ожиданий в полной мере, недостатки каждого из
подходов были описаны выше, так что появилась необходимость найти более перспективный вариант
для дальнейшего исследования. В результате в статье \cite{graph_mashable} %% Не уверена, стоит ли тут упоминать эту статью.
было найдено упоминание об использовании графа знаний. Идея использовать его для работы с Википедия
кажется интересной и перспективной.

В отличие от многих способов представления естественного языка использование графа знаний помогает
в большей мере уделить внимание семантической составляющей, нежели синтаксической. Одними из основных
преимуществ представления языка именно в~форме подобного графа являются: возможность отобразить семантическую
структуру наиболее полно, используя при этом ограниченный и сравнительно небольшой набор отношений, а так же
сымитировать ход мысли, наиболее близкий к~человеческому.
\subsubsection {Описание графа знаний}
Граф знаний \cite{knowledge_graph} есть ориентированный(в большинстве случаев) граф, в~котором 
вершины представляют так называемые концепты (токены и типы),
а ребра --- отношения между ними, причем отношения могут быть как бинарные, так и многомерные. 

\textsl{Токену} соотвествует какой-либо реальный объект или абстрактное понятие, на графе он обозначается
как $\Box$. Так же можно сказать, что токен есть результат восприятия объекта реального мира
или понятия, существующего только в сознании конкретного человека.

С точки зрения различных людей одним и тем же объектам реального мира могут соответствовать различные токены.
Таким образом, токен, отражающий нечто существующее для одного человека, может не существовать в сознании другого
вообще. Но если он существует для большинства, то такой токен отражает некоторое общее понятие и в терминологии
графа знаний называется \textsl{типом}.


Как было сказано, ребра графа отражают отношения между токенами или между токеном и признаком, причем
признаком чаще всего является слово. 

Определено восемь типов бинарных отношений:

\begin{itemize}

\item{
Равенство (Equality): EQU

EQU-отношение между двумя токенами свидетельствует о том, 
что данные токены идентичны.
}

\item {
Отношение включения (Subset relationship): SUB

}

\item{
Сходство (Similarity of sets, alikeness) : ALI

ALI-отношение используется для того, что указать на наличие общих
элементов у двух токенов.
}

\item{
Несходство(Disparateness) : DIS

DIS-отношение используется для того, чтобы указать на то, что
два токена соответствуют совершенно несвязанным понятиям.
}

\item{
Причинная связь (Causality) : CAU

СAU-отношение служит для выражения причинно-следственной связи или
указания того, что один объект влияет на другой. 
}

\item{
Отношение порядка (Ordering) : ORD  
}

\item{
Отношение атрибутивности (Attribution) : PAR
}

\item{
Информационная зависимость (Informational dependency) : SKO

Данный вид отношений лучше пояснить на примере. Если есть слово "saying"(1)
и фраза "what is said"(2), то (2) будет изменяться в соответствии с (1), то есть
информационно зависеть.
}
\end{itemize}

Стоит подробнее обсудить некоторые из перечисленных выше отношений. 

Начнем с примера. Предположим, речь идет о пуделе Плуто (Pluto poodle),
 тогда получим следующий граф:

Pluto $\xrightarrow{EQU} \Box \xleftarrow{ALI}$ poodle 

В данном случае Pluto, $\Box$ и poodle являются \textsl{метками(marks)}, очевидно, несущими 
какой-то смысл. Символу $\Box$ сооветствует токен. Отношение ALI между токеном и меткой указывает
на то, что poodle --- это тип и что данный токен принадлежит типу. Отношение EQU служит для выражения
того факта, что Pluto есть некоторое значение, присваиваемое данному токену, иначе говоря, указывает,
что Pluto --- конкретный "экземпляр", соответствующий описанному токену. Исходя из этого, нетрудно заключить,
что имеется в виду собака породы пудель (то, что пудель -это собака можно увидеть на графе для слова
 poodle) и что Плуто --- конкретный представитель такого типа собак.

Отдельно необходимо поговорить о SUB-отношениях между двумя токенами. Каждый токен отражает некоторое 
понятие, причем в данном случае понятие есть множество некоторых свойств. Если одно из этих  множеств 
является подмножеством другого, то имеет место SUB-отношение между двумя токенами.
 
Очевидно, что, используя только бинарные отношения, невозможно описать все то, что происходит в мире.
Для решения этой проблемы были введены n-арные отношения. 

Существует четыре типа n-арных отношений:

\begin {itemize}

\item{
Фокусировка на состоянии: FPAR
}

\item{
Отрицание состояния: NEGPAR
}

\item{
Возможность состояния: POSPAR 
}

\item{
Необходимость состояния: NECPAR
}
\end{itemize}

На графе этот тип отношений обозначается рамкой (frame), которая представляет собой  узел с меткой. Внутри рамки %% не уверена, что frame стоит переводить. может, лучше сделать кальку с английского?
находится некоторый граф.
Изначально существовал только один тип n-арных отношений --- FPAR, который используется для выражения логического "и".
Так же FPAR-отношения служат для того, чтобы показать, что некоторый подграф есть часть одного большого графа. Разница
между SUB- и FPAR-отношениям заключается в том, что первый тип имеет место, когда нужно выразить отношения между двумя
множествами, а второй, как сказано выше, --- между графом и подграфом.

%%картинка с примером таких отношений
 

\subsubsection {Логические слова}
Основной единицей в работе с естественным языками является слово, именно поэтому прежде всего необходимо научиться 
строит графы для слов, из которых при объединении можно будет получить граф предложения.

В данной книге предлагается концепция "логических слов", то есть слов, выполняющих логические функции в предложении
или даже абзаце.

Выделяется два типа логических слов в соответствии с типами отношений:
\begin{itemize}

\item{
Логические слова первого типа –-- слова, граф которых содержит один из четырех типов n-арных отношений.
}

\item{
Логические слова второго типа –-- слова, граф которых содержит в качестве основной связи один из восьми типов 
бинарных отношений
}
\end {itemize}

С использованием этого разделения строятся "каркасы" графов слов для каждого типа. 


\subsubsection {Построение графа знаний}
Для того, чтобы получить граф предложения необходимо, во-первых,  знать значения слов,
во-вторых, знать, какие функции в предложении они выполняют. Для этого строят два типа графов
слов: семантический и синтаксический.

В английском языке выделяют восемь типов слов: существительное~(N), глагол~(V), прилагательное~(adj),
местоимение (PN),  числа и числительные~(num), determiner (подразумевая артикли:~the,~a), предлог~(prep),
наречие~(adv).

Сначала для каждого типа слов строится синтаксический граф, отражающий функцию в предложении.
Семантический граф получается из синтаксического следующим образом. Сначала предложение
разделяется на кусочки(chunks), и для каждого из этих кусочкой строится семантический граф. Для того,
чтобы правильно разбить предложение на части используются так называемые "индикаторы разделения"
(chunk indicators). Одним из таких индикаторов является, к~примеру, пара запятых, тире. Есть еще 
индикаторы, согласно которым полученные части могут быть разбиты на подчасти.
После этого конструируются семантические графы для каждого слова из кусочка и соединяются.
Граф предложения получается путем соединения этих кусочков, причем в том порядке, в котором
они даны.

\subsection{Определение семантической близости понятий с~помощью графа знаний}
В рамках данной научной работы был реализован алгоритм определения близости слов
на основании их положения в дереве понятий, представленном в семантической сети
английского языка Wordnet\cite{wordnet}.
Были проведены запуски на парах слов, упомянутых в~\cite{complexSim},
для которых уже известна средняя оценка близости, данная людьми.

Wordnet\cite{wordnet} --- лексическая база данных английского языка,
разработанная
в Принстонском Университете и выпущенная вместе с сопутствующим
программным обеспечением.
Основной единицей Wordnet является не отдельное слово, а так
называемый синсет (синонимический ряд),
объединяющий в себе множество слов, имеющих схожий смысл, выражающих
одно и то же понятие.
Кроме набора синсетов в базе представлены также и семантические
отношения между ними,
которые позволяют определить понятия-антонимы рассматриваемого концепта,
а так же его гиперонимы --- более общие понятия, отражающие надмножество
к исходному.
Например, для понятия "собака" гиперонимами могут являться такие концепты,
как "домашнее животное" или "семейство собачьи".
Очевидно, что на основе отношений-гиперонимов можно построить
ациклический ориентированный граф --- дерево, вершинами которого
будут отдельные понятия-синсеты, а направление дуг указывает на
гиперонимы к ним.
Кроме того, следует упомянуть о существовании базового концепта, являющегося
гиперонимом ко всем остальным синсетам. На графе он будет корнем дерева,
из которого не выходит ни одна дуга и до которого существует путь от
каждой вершины.

Автором был реализован алгоритм, впервые описанный  в статье
\cite{inproceedings}
и рекомендуемый к реализации в \cite{complexSim}.
Для вычисления величины, определяющей близость пары понятий, предлагается
воспользоваться следующей формулой:


$$ Sim(w_1, w_2) = \frac{ 2\times d(l, r) }
	               { d(w_1, l) + d(w_2, l) + 2\times d(l,r), }$$

где $w_1,w_2$ --- рассматриваемые концепты, 
$l$ --- их~наименьший общий предок в~дереве гиперонимов,
$r$ --- базовое понятие --- корень дерева, 
а $d(n_1,n_2)$ -- 
кратчайшее расстояние между вершинами $n_1$ и $n_2$.

Как видно из построения, полученная величина нормирована: ее значения лежат
в интервале [0,1].
Причем ее значения зависят от следующих факторов:
1. Как далеко в дереве расположены друг от друга сравниваемые концепты .
 (Чем ближе расположены, тем больше величина близости)
2. Насколько далеко от корня расположен наименьший общий предок.
  (Чем дальше расположен, тем больше значение)
Расстояние от корня до понятия, по сути, определяет его специфичность:
чем больше расстояние, тем уже рассматриваемый концепт.
Таким образом, даже если расстояние между сравниваемыми вершинами
относительно велико, но рассматриваемые понятия принадлежат
достаточно узкому классу, т.е. их общий предок расположен далеко от корня,
в связи с  этим вычисленная таким образом мера близости будет
сравнительно велика.

\subsubsection{Особенности реализации}
В качестве языка программирования для реализации был выбран Java,
а для взаимодействия с базой Wordnet использовалась библиотека JWNL\cite{jwnl}.
Эта библиотека дает удобные инструменты для работы с отношениями на синсетах,
и такие задачи как нахождение общего предка, расстояния между ними, не
вызвали затруднений.

Однако, при запуске на упомянутом выше тестовом наборе было замечено
завышение оценок
по сравнению со средними оценками людей.
Этого удалось избежать, снизив коэффициент "2" перед членом формулы d(l,r)
до значения 0,8. Вероятно, коэффициент "2" был рассчитан на
семантические сети меньшего
размера, чем Wordnet, где длина пути от корня до достаточно общих
понятий сравнительно велика.

Кроме того у  автора возникли сложности, связанные с многозначностью
слов английского языка.
Даже при верном определении части речи слова возможна ситуация, когда
оно может присутствовать
в разных синсетах, причем в зависимости от того, какой из них будет выбран,
итоговое значение меры близости может иметь разброс от близкого к нулю
до единицы.
Например, в случае сравнения глаголов "to go" и "to move" ,  первый
можно перевести как "уходить",
и как "начать что-либо", и, в зависимости от выбранного смысла, они
могут быть как очень похожи,
так и иметь совершенно разный смысл.
В таких случаях верное значение слов можно получить только из
контекста, но это выходит
за рамки решаемой задачи: на вход алгоритму подается только само слово
и его часть речи.
Другим решением этой проблемы может стать нахождение некоторого статистичского
среднего значения близости из выборки, составленной из значений меры
близости для всех пар синсетов,
а также возможен вариант выбрать те синсеты, при которых
эта мера достигает своего максимума.
В соответствии с паттерном проектирования "Стратегия" \cite{strategy}, был выделен
отдельный интерфейс
AbstractSelectionEvaluationStategy для определения итогового значения
меры близости, использующий
в качестве входных данных массив значений меры для всех пар синсетов.
Были созданы несколько его реализаций, которые выбирают:
\begin{enumerate}
\item{Среднее арифметическое выборки}
\item{Максимальное значение из всей выборки}
\item{Статистическую медиану выборки (Может использоваться для
нормального распределения)}
\end{enumerate}

В качестве базовой стратегии было зафиксировано использование реализации с
максимальным значением,
но предполагается, что в дальнейшем выбор стратегии будет усложнен.

\subsubsection{Результаты}
Как уже было сказано выше, были произведены замеры для 97 пар слов
с данными для них оценками семантической близости. Результаты в виде таблицы
представлены в приложении к работе.
Среднее значение отклонения оценок людей и алгоритма составляет 0.150776,
что можно считать удовлетворительным результатом.
Наибольшее отклонение - 0.55947 - было достигнуть для пары слов
"fruit" и "food" ,
где оценка данная людьми была равна 0.77, а значение, предложенное алгоритмом,
составило 0.210526. Заниженная в данном случае оценка связана с несовершенством
базы Wordnet, где понятию "fruit" соответствует только значение "плод растения" ,
когда с точки зрения большинства людей наиболее важным свойством фрукта
является возможность употребления его в пищу. То есть для многих людей
близким гиперонимом понятия "фрукт" является "еда" , о чем и говорит оценка 0.77.









