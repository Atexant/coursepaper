\section{Задача построения базы данных на основе Wikipedia}

Data Mining на русский язык можно перевести как ``извлечение данных'',
хотя, насколько можно судить, общепринятого перевода в русскоязычной
академической литературе пока нет. Далее в тексте будем использовать
английский вариант названия или заменять его сокращением DM. 
DM охватывает подходы к обработке всех видов данных, включая графические и звуковые. 
Мы же ограничимся более узкой сферой, посвящённой
обработке только текстовой информации, называемой Text Mining (TM).
TM в~своём развитии прошёл несколько фаз \cite{fixme}, о~которых будет сказано ниже. 
Наиболее перспективный современный подход связан с попыткой разработки методов, обладающих
способностью выделять понятия или, как иногда говорят, сущности, обозначаемые словами в тексте. 
Машина при этом всё равно не способна 
понимать смысл текста, но можно попытаться определить множество
сущностей, их атрибуты и связи между ними. Понятия для компьютера
остаются "безликими", это не более чем отдельные переменные в
программном коде, но этого может быть достаточно, чтобы получить
удобные инструменты обработки информации нового
поколения. Актуальность и перспективность такого подхода подтверждает
пристальный интерес к этой технологии со стороны крупных корпораций,
таких как Google \cite{fixme}.

Построение множества понятий, упоминаемых в тексте, значительно
затруднено большим количеством имён собственных различных персоналий,
местностей и пр. Легко понятные каждому человеку имена и названия,
такие как Москва, Ньютон, Платон, для компьютера остаются
недоступными. Каких-либо методов связать их с понятиями ``человек'' или
``город'', кроме как иметь в своём распоряжении заранее готовую базу
данных (БД), видимо, практически нет, по крайней мере, в настоящую эпоху. 
По этой причине весьма интересный потенциал представляет собой
свободная энциклопедия Wikipedia.

%%АFIXME:Здесь немного выходит неоконченная мысль, надо попытаться свести к каким-нибудь практическим примерам.

\section{Обзор алгоритмов Data Mining}

В процессе своей эволюции DM прошёл несколько этапов развития.
 Ранние методы в~основном представляли собой различного рода эвристические алгоритмы.
Их суть сводилась к вычислению некоторой оценки, получаемой
при~обработке текста, на основе которой принималось решение. К ним
относятся алгоритмы классификации и кластеризации, кратко описанные ниже. 
Более развитые алгоритмы, составляющие группу методов извлечени информации, 
позволяют прослеживать различные логические взаимосвязи и выделять сущности на основе содержания текста. 
Их отличительная особенность в том, что в них присутствует в той или иной форме понятие
``объекта'', с которым связано множество атрибутов и их значений.

\subsection{Классификация}

Классификация является одной из самых простых и распространенных задач DM. 
Анализируется множество документов $D$ и в результате каждому документу
ставится в соответствие один из классов$L$.  На начальном этапе
задается обучающий набор данных, для элементов которого уже
определена принадлежность к классу. 
Затем выбирается алгоритм классификации. 
Точность его работы определяется следующим образом:
обучающий набор данных классифицируется, а результат сравнивается с заданным заранее эталоном.
Если результат работы алгоритма проходит некоторый необходимый уровень совпадений с~эталоном, 
то с его помощью выполняется классификация остальных данных.  
Для~примера рассмотрим некоторые конкретные варианты классификации.

\subsubsection{Vетод ближайших соседей}

Данный алгоритм является одним из наиболее детально изученных и эффективных среди
алгоритмов классификации текстов.
Для определения класса обрабатываемого документа предлагается использовать 
его близость с документами из обучающего набора. Таким образом, класс обрабатываемого
документа может быть получен на основании принадлежности к классам близости документов
из обучающего набора. 
Данный подход также известен, как ``Метод $k$-ближайших соседей'', 
если для рассмотрения выбирается $k$ документов.
Существует множество различных способов подсчета близости между двумя документами.
Один из самых примитивных заключается в подсчете числа общих слов в каждом документе.
Очевидно, что результат нужно нормализовать, так как тексты могут быть разной длины.
Но в этом случае необходимо учитывать, что одни и те же слова могут иметь множество
различных значений, что приводит к варьированию информационного содержимого.
Другой стандартный способ основан на применении косинусной меры близости. Для этого пространство 
документов преобразуется в векторную модель: каждый документ представляется в виде
$m$-мерного вектора 
($m$ --- размер обучающего набора), %%FIXME:размер - не очень однозначно, можно понимать и как количество классов, и как количество самих документов.
т.~е. документ $d$ можно представить
 в виде числового вектора признаков: 

$$w(d) = (x(d,t_1),\dots, x(d,t_m)),$$ 

%%Валентина!!! Расшифровка обозначений!!! m, t_k, x() - всё надо прокомментировать.

причем каждому элементу
вектора соответствуют слова (группа слов). 
Таким образом, размер вектора определяется количеством 
лов (групп слов).

Для определения степени значимости слова в выбранном документе из заданной коллекции
используется такое понятие, как ``вес''. 
Большой вес соответствует словам, часто встречающимся 
в нужных документах, но редко во всей коллекции. 
Таким образом, вес $w(d,t)$ для слова $t$
из документа $d$ расчитывается как произведение частоты употребления слова (term frequency) %%тут было "собственная частотность". 
и инвертированного значения частоты, с которой это слово встречается в документе.
%%FIXME: Тут всё-таки несколько непонятно выходит. Чем отличается "собственная частотность" и "частота, с которой встречается"  ?

Чтобы полученное произведение не зависело от длины документов, необходима нормализация.
[здесь будет формула]
Вес должен быть равен нулю, если данное слово не встречается в документе.

Если документ $d$ может быть представлен в~виде вектора, состоящего из значений весов слов, 
то близость двух документов $d_1$ и $d_2$ определяется, как следующее скалярное произведение векторов:

[тоже формула]

Для~определения принадлежности документа $d_i$ некоторому классу $L_m$ 
должна быть определена близость $S(d_i,d_j)$ для  всех документов $d$ из~обучающего набора. %%Тут было d_j, но по смыслу не подходит, или всё-таки  должно быть d_j?
Затем выбирается $k$ документов, наиболее близких  к заданному $d$, назовём их соседями. 
Количество экземпляров класса среди соседей можно использовать как вероятность принадлежности данного документа к этому классу: 
документу определяют принадлежность к классу, наиболее широко представленном среди соседей. 

При выборе числа соседей $k$ необходимо учитывать, что при $k = 1$ алгоритм неустойчив к шумовым
выбросам: он даёт ошибочные классификации не только на самих объектах-выбросах, но и на 
ближайших к ним объектах других классов. 
Но при $k = m$ алгоритм чрезмерно устойчив и вырождается 
в константу. Таким образом, крайние значения  нежелательны. На практике оптимальное значение
параметра  определяют по критерию скользящего контроля, чаще всего — методом исключения объектов
по одному (leave-one-out cross-validation) \cite{machinelearning} %%использован www.machinelearning.ru 

\subsubsection{Деревья принятия решений}

Деревья принятия решений --- стандартный инструмент для решения задач классификации в DM. 

Деревья решений представляют собой классификатор, состоящий из набора правил, применяемых последовательно. 
Лучше всего проиллюстрировать принцип работы деревьев решений в ходе процесса
обучения, на начальном этапе которого работа производится с полным обучающим набором.
Данный алгоритм использует стратегию ``разделяй и властвуй'': 
выбирается некоторое слово $t_i$ из обучающего набора $М$, 
которое наиболее точно может указывать на принадлежность документа к классу. 
На следующем шаге обучающий набор $M$ разделяется на два подмножества:
подмножество $М_{i+}$ , состоящее из документов, содержащих выбранное слово $t_i$, и $M_{i-}$, 
состоящее из документов, не содержащих данное слово. Данная процедура рекурсивно повторяется для $M_{i+}$ и $M_{i-}$
до тех пор, пока не получится подмножество, состоящее из документов одного класса. В результате
получаем дерево разбиения, в~листьях которого записаны требуемые классы.

\subsection{Кластеризация}

Кластеризация является задачей, сходной с классификацией, но более сложной. 
На вход так же поступает массив 
документов $D$, но в данном случае классы заранее не определены, и суть состоит в разделении исходных
данных по группам на основании их близости. 
Причем из одного кластера объекты должны быть более близкими,
а из разных --- менее.

\subsubsection{Алгоритм $k$-средних}

Одним из наиболее часто используемых алгоритмов кластеризации является алгоритм $k$-средних ($k$-means). 
Его распространенность обусловлена простотой реализации и возможностью работать с большими объемами информации.
Помимо этого  алгоритм показывает
хорошие результаты именно применительно к кластеризации текстов. 

В~ходе работы алгоритма на первом шаге определяется число кластеров $k$. 
Затем случайно выбирается $k$ документов --- начальных центров кластеров, иначе центроидов 
(центроид --- это вектор, элементы которого представляют собой средние значения признаков, вычисленные по всем записям кластера). 
Документы распределяются по кластерам в зависимости от близости к тому или иному центроиду. 
Затем центроиды пересчитываются для каждого кластера. 
Процесс продолжается до тех пор, пока значения центроидов изменяются. 
Так же можно задать число итераций, по достижении  которого работа алгоритма завершается.

\subsubsection{Альтернативные подходы к кластеризации}

Помимо этого классического алгоритма, существует ряд других, которые можно назвать альтернативными.
К примеру, алгоритм ``совместной кластеризации'' (co-clustering). 
Он представляет собой совершенно
иной подход к кластеризации, заключающийся в синхронной кластеризации документов и слов.

Идея другого алгоритма, называемого ``алгоритм нечеткой кластеризации'', состоит в том, что в результате
генерируется не четкое разделение по кластерам, как в случае классических алгоритмов, а набор степеней принадлежности
документа к тому или иному кластеру.

\subsection{Построение тезауруса для Wikipedia}

Предпринимались попытки применения описанных идей к~обработке материалов Wikipedia.
Интересный пример можно найти в~статье \cite{thesaurus}.
её авторы предлагают алгоритм построения тезауруса --- словаря, отражающего близость между словами.
В качестве корпуса для извлечения знаний авторы используют множество статей Wikipedia,
принимая во внимание такие сильные её стороны, как большое число 
статей, регулярные обновления (live updates), большое количество ссылок между статьями 
(a dense link structure), короткий текст ссылки и идентификация каждого понятия по URL.
Подобные попытки уже предпринимались, но, по словам авторов, ни в одной из них не обращалась
должного внимания на улучшение точности и масштабируемости. Так что их задачей стало то же 
построение тезауруса, но уже со стремлением улучшить вышеописанные характеристики.

В качестве входного материала используется множество статей, причем статьи рассматриваются как понятия. 
Такое возможно, поскольку каждая статья отражает объекты реального мира и различные понятия.

В результате работы алгоритма производится построение словаря (тезауруса), состоящий из множества статей (понятий) и гиперссылок между ними.
тезаурус может быть представлен в виде графа $G = \{V,E\}$, где 
$V$  --- это множество статей
а $E$ ---это множество ссылок.
%%FIXME:Тут выходит смысловое противоречие: если рёбра в графе - ссылки, это просто граф ссылок, но не тезаурус.
Близость двух статей (понятий) $v_i$ и $v_j$  в~сильной степени зависит от двух параметров:

\begin{itemize}

\item{
количество путей между $v_i$ и $v_j$;
}

\item {
длины каждого пути между $v_i$ и $v_j$.
}

\end{itemize}

Путь между двумя статьями --- это последовательность ссылок 
(количество таких ссылок определяет длину пути),
которая соединяет данные статьи. 
Очевидно, что таких путей может быть несколько. 

Близость тем больше, чем больше количество путей между соответствующими статьями. 
Аналогично, если статьи в графе расположены рядом и содержат гиперссылки на
одни и те же статьи, то их близость будет больше, чем у более  отдалённых статей.

Важную роль играет количество входящих ссылок. 
Рассмотрим случай, когда существует некоторая статья, на которую ссылается большое число других статей. 
Такая статья имеет большое количество ``коротких'' путей, что в свою очередь означает,
что она тесно связана со~ссылающимися на нее статьями, 
т.~е. понятия, соответствующие этим статьям, являются достаточно близкими. 
Однако если обратить внимание на число входящих ссылок (в данном случае оно велико), 
то понятия, соответствующие подобным статьям, можно обозначить как общеупотребительные,
и в большинстве случаев они не очень значимы. 

Говоря о сложности нахождения значения приведенных выше параметров, стоит заметить, что
задача подсчета количества входящих ссылок является тривиально решаемой, но для 
подсчета длин путей между статьями придется прибегнуть к нестандартным подходам. 
Для подсчета количества путей используется степень матрицы смежности графа, 
построенного на основании наличия ссылки между двумя соответствующими статьями. 
Степень --- это максимальная рассматриваемая длина пути. В результате возведения матрицы
%%FIXME:Правильно ли тут назвать ребро? Было понятие "элемент", но это слишком неопределённо.
в степень в каждом ребре $(V_i, V_j)$ мы получаем суммарное количество путей между статьёй $V_i$  и статьёй $V_j$,
длина которых не больше максимальной. 

На словах данные действия кажутся достаточно простыми, но при реализации могут возникнуть сложности. 
Использовать матрицу смежности и стандартную операцию умножения не стоит, потому что,
к примеру, Wikipedia содержит около 1.3 миллиона статей, 
следовательно,  только для хранения данных может понадобиться несколько терабайт дискового пространства. 
Помимо этого, для выполнения операций умножения
понадобится чрезмерно много времени, т.~к. трудоемкость операции составляет $O(n^3)$.
Тем не менее, поскольку матрица смежности разрежена, и большая часть ее элементов --- нули, 
для обеспечения масштабируемости можно использовать специальные сжимающие структуры данных и различные методы анализа данных. 
В связи с этим авторы предлагают использовать такую структуру данных, как двойное бинарное дерево (Dual binary tree) и алгоритм умножения, 
учитывающий особенности подобной структуры.
Для достижения эффекта сжатия в бинарных деревьях хранятся только ненулевые элементы матрицы 
смежности. Двойное бинарное дерево состоит из двух типов бинарных деревьев: 
$i$-дерево и $j$-дерево.
Каждый элемент $i$-дерева соответствует строке матрицы смежности и содержит ссылку на корень $j$-дерева.
Следовательно, двойное бинарное дерево представляет собой $n+1$ бинарное дерево.  %%FIXME:Тут опять по смыслу как-то нечётко. Нужно бы написать конкретней.
В результате использования подобной структуры запись и чтение данных производится очень быстро, так как
трудоемкость составляет всего $O(\log n)$ в обоих случаях.
%%FIXME:Next, we define the multiplication algorithm for the DBT as follows:
Алгоритм умножения для подобной структуры представлен ниже:

%%1 for i belongs i-Tree
%%2   for j belongs j-Tree(i)
%%3     for k belongs j-Tree(j)
%%4       Ri,k := Ri,k + aj,k • ai,j ;


Функция $JTree(i)$ извлекает все элементы из $i$-ой строки матрицы смежности $A$.
$A_{jk}$ --- это элемент $j$-ой строки и $k$-го столбца матрицы. 
Количество итерацией первого цикла равняется $N$, для последующих циклов это число будет стремиться к среднему 
количеству ссылок в статье --- $M$. 
Таким образом, общее число шагов будет равно $O(M2NLonN)$. %%FIXME:Валентина, напишите пожалуйста формулу с пробелами между обозначениями, чтобы правильно вклинить в tex.
%%[22:42:50] Денис Жарков: 
Число $M$ находится в интервале от 20 до 40 в случае статьи ``Википедия'', %%FIXME:Тут имеется в виду какая-то конкретная статья?
что значительно меньше размера исходной матрицы $n$. В итоге, мы получаем результат, 
сохраненный в другом двойном бинарном дереве $R$.

Одним из основных минусов описанного выше алгоритма построения тезауруса можно считать тот факт,
что ничего не говорится о нахождении антонимов, хотя они являются неотъемлемой частью подобного словаря.
Что касается синонимов, то их нетрудно можно найти из связей между статьями, введя желаемый порог близости.

\subsection{Извлечение информации}

Подходы на основе классификации и кластеризации не представляют сейчас
большого интереса, поскольку уже достаточно хорошо изучены и обладают
относительно слабым потенциалом для дальнейшего развития. 
Более перспективными в этой ситуации являются задачи, связанные с
извлечением информации.
Тексты на естественном языке содержат значительное количество информации,
которая напрямую не поддается автоматической обработке. Однако, можно использовать
возможности компьютера для работы с большими массивами текстов и извлечения
необходимой информации из отдельных слов, предложений или частей текста.
Таким образом задача извлечения информации заключается в получении структурированной
информации из неструктурированного текста на естественном языке и может рассматриваться
как отдельная часть общей задачи понимания естественных языков. 

Задача извлечения информации включает в себя следующие подзадачи:
1. Распознавание именованных сущностей.
   Эта подзадача в свою очередь разделяется еще на несколько более мелких.
   
   Непосредственно извлечение именованных сущностей, к примеру, названий географических
   объектов, людей, организаций и прочего.
   
   Разрешение анафор и кореференций(Coreference), то есть обнаружение связей между уже
   извлеченными именами и названиями. К примеру, "Iternational Business Machines"
   и "IBM" соответствуют одной и той же реалии. Или в случае двух предложений
   "George Harrison wrote a new song. He was excited of it." необходимо, чтобы
   местоимение "he" ассоциировалось с прежде полученным "George Harrison".
   
   Распознавание отношений между словами в предложении. К примеру, из предложения
   "Mick Jagger plays in the Rolling Stones" нужно выделить следующее отношение:
   ЧЕЛОВЕК играет в ГРУППЕ.
   
2. Извлечение информации из слабоструктурированных текстов. 
   Примером этой задачи может служить нахождение таблиц в документе и последующая
   работа с ними.
   
3. Нахождение терминологии, специфичной для данного документа.

Одним из возможных способов представления текста для машинной обработки является
представление его в виде объектов с атрибутами, т.е. извлекаются части текста и 
и для них обозначаются атрибуты.

Вышесказанное можно проиллюстрировать на примере. Дано предложение на 
естественном языке: "Robert L. James, chairman and chief executive
officer of McCann-Erickson, is going to retire on July 1st.  He will
be replaced by John J.Donner, Jr., the agencies chief operating
officer."  Получим следующие атрибуты: организация (McCann-Erickson),
должность (chief executive officer), дата(July 1), имя уходящего с
должности человека (Robert L. James), имя приходящего на должность
человека (John J. Donner, Jr.).

При реализации этого подхода можно столкнуться с некоторыми сложностями. 
К примеру, при составлении базы данных, элементами которой буду объекты и
атрибуты, придется выбирать фиксированное число атрибутов для представления
текста, что тоже представляется достаточно сложной задачей.

Другим интересным способом представление текста в формате, пригодном для
автоматической обработки, является использование так называемого графа знаний - 
специфического графа, отражающего связь между словами и группами слов в предложении.

/subsection{WEKA}

По причине недостатков подхода, основанного на построении тезауруса, было принято решение перейти к 
изучению более эффективных и перспективных методов ТМ. Внимание на себя обратил ныне активно
развивающийся проект Weka, о котором упоминалось в статьях(?) Google [ссылка].
Weka представляет собой свободно распространяемый пакет для анализа данных, 
написанный на языке Java в университете Уайкато (Новая Зеландия). Weka позволяет 
выполнять такие задачи анализа данных, как классификация, кластеризация, извлечение 
правил ассоциаций и выборка атрибутов. 


Классификация и кластеризация не представляют интереса в силу своей изученности в отличие от
последних двух, которые в соответствии с общей классификацией методов ТМ можно отнести к задачам
извлечения информации.

Алгоритмы извлечения правил ассоциаций и выборки атрибутов являются развитыми алгоритмами,
для них данные об объектах хранятся в виде фиксированного списка атрибутов. Множество объектов
составляет базу данных (БД). Приведём пример подобной базы данных:
•	множество объектов - множество видов животных;
•	вариант множества атрибутов: вид (str),средний вес (float), кормление молоком (bool),
наличие шерсти (bool), длина названия вида (int).

Для "Википедия" подобная база данных может выглядеть следующим образом: в качестве множества
объектов рассматривается множество статей, а атрибутами являются, к примеру, 1000 самых часто
встречаемых и информативных понятий(?). 

Извлечение правил ассоциаций заключается в поиске интересных и полезных закономерностей в БД.
Сказанное выше стоит проиллюстрировать на примере (БД описана выше). Выберем два атрибута: средний 
вес и наличие шерсти, можем получить следующее логическое выражение: если средний вес равен 
50 кг, и есть шерсть, то вид - волк или шимпанзе, и т. д.

Выборка атрибутов - задача сортировки атрибутов объектов по их информационной содержательности.
Проиллюстрируем данную задачу на примере, используя БД, описанную выше. В БД после работы 
алгоритма выборки атрибутов должны получить список: вид, средний вес, кормление молоком, 
наличие шерсти. Здесь нет атрибута “длина названия вида”, т.к. он, очевидно, не несет
никакой информационной ценности.

Задача выборки атрибутов и извлечения правил ассоциаций являются интересными и перспективными
для изучения. [переформулировать?]

В результате более детальной работы с Weka выяснилось, что подход для решения интересующих задач
достаточно поверхностный и не рассчитан на работу с большим количеством атрибутов. Алгоритмы, 
представленные в Weka, - эвристические, т. е. даже если бы они были приспособлены для работы с 
большим количеством атрибутов, то в результате позволили ли бы лишь приблизиться к цели, а не
достичь ее.
