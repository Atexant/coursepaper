\documentclass{beamer}

\usepackage[russian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{cmap}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{psfrag}
\usepackage{amsfonts}
%%\usepackage{amssymb}

\newcommand{\MARK}[1]{{\bf {\it #1}}}
\newcommand{\CODE}[1]{{\ttfamily #1}}

\setbeamertemplate{footline}[frame number]
\usecolortheme{seahorse}
\beamertemplateshadingbackground{white}{blue!3}

\begin{document}
\begin{frame}
\begin{center}
Кирюшкина Валентина \\
\vspace{1cm}
{\Large Исследование алгоритмов построения базы данных для методов Data mining на основе материалов свободной энциклопедии “Википедия”}\\ 
\end{center}
\end{frame}

\begin{frame}
\frametitle{Data mining и Text Mining}
Data mining – процесс нахождения полезных закономерностей (useful patterns)  в большом наборе данных.\\
\vspace{1cm}
Text mining - процесс выделения полезных закономерностей
(useful patterns) в больших массивах текста на естественном
языке. Text mining = Text Data Mining.
\end{frame}

\begin{frame}
\frametitle{Задача построения базы данных на основе Wikipedia}
Задача: Выделить понятия (сущности), обозначаемые словами в тексте.\\
\vspace{1cm}
Сложности:
\begin{itemize}
\item{Большое количество имен собственных}
\item{Потребность связыания имен собственных с объектами реального мира}
\end{itemize}
\vspace{1cm}
Пример. Такие названия, как Москва, Ньютон, Платон необходимо уметь распознавать,
а так же связывать их с понятимя "город" или "человек". \\
\vspace{1cm}
Решение: Необходимо иметь заранее готовую базу данных. По этой причине
весьма интересный потенциал представляет собой свободная энциклопедия
Wikipedia.\\
\vspace{1cm}
Важные особенности:
\begin{itemize}
\item{Большое число статей}
\item{Регулярные обновления}
\item{Большое количество ссылок между статьями (a dense link structure)}
\item{короткий текст ссылки и идентификация каждого понятия по URL}
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Основные направления Data mining}
\begin{enumerate}
\item{Классификация}
\item{Кластеризация}
\item{Извлечение информации}

\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Классификация документов}
$$D \mapsto L, L \in \mathbb{L}$$
\vspace{1cm}
$\mathbb{L}$ - множество категорий документов.\\
\vspace{1cm}
В отличие от кластеризации множество категорий задано изначально.
\vspace{1cm}
Дан обучающий набор образцов документов, для которых известно, какому классу они принадлежат.
\end{frame}

\begin{frame}
\frametitle{Кластеризация для документов}
$$D \mapsto P, P \in \mathbb{P}$$
\vspace{1cm}
$\mathbb{P}$ - множество кластеров.\\
\vspace{1cm}
Документы разделяются на подмножества таким образом, что:

\begin{itemize}
\item{внутри одного подмножества документы \underline{однородны}}
\item{документы из разных подмножеств \underline{разнородны} }
\end{itemize}
\vspace{1cm}
Dist($d_{1}$,$d_{2}$) - мера расстояния между документами.
\end{frame}

\begin {frame}
\frametitle{Построение тезауруса}
Задача: Построение тезауруса - словаря, отражающего семантические отношения
между словами, в данном случае - близость между словами.\\
\vspace{1cm}
Входной материал: Множество статей, которые рассматриваются как понятия.\\
\vspace{1cm}
Выходной материал: значение функции, аргументами которой являются
две статьи, соотвествующие понятиями, степень схожести между которыми
необходимо узнать.
\end{frame}

\begin{frame}
\frametitle{Алгоритм построения тезауруса}
Так как Викикпедия состоит из множества статей (понятий) и гиперссылок между ними, то может быть представлена
в виде графа $G = \{V,E\}$, где $V$  --- это множество статей, а $E$ ---это множество ссылок.\\
Близость между понятиями(статьями) $v_i$ и $v_j$ завист от следующих параметров:

\vspace{1cm}

\begin{itemize}
\item{количество путей между $v_i$ и $v_j$}
\item{длины каждого пути между $v_i$ и $v_j$}
\item{количество входящих ссылок}
\end{itemize}

\vspace{1cm}
Путь между двумя статьями — это последовательность ссылок (количество таких ссылок определяет длину пути), которая соединяет данные статьи.\\

\end{frame}

\begin{frame}
\frametitle{Извлечение информации}
Задача извлечения информации заключается в получении структурированной информации из
неструктурированного текста на естественном языке.\\

\vspace{1cm}
Подзадачи:
\begin{enumerate}
\item{Распознавание именованных сущностей}
\item{Извлечение информации из слабоструктурированных текстов}
\item{Нахождение терминологии, специфичной для данного документа.}
\end{enumerate}


\vspace{1cm}

Данные могут быть прелставлены в виде объектов с атрибутами, т.е. извлекаются 
части текста и и для них обозначаются атрибуты.\\

\vspace{1cm}
Пример. Дано предложение на естественном языке: "Robert L. James, chairman and chief executive
officer of McCann-Erickson, is going to retire on July 1st. He will be replaced
by John J.Donner, Jr., the agencies chief operating officer."Получим следующие
атрибуты: организация (McCann-Erickson), должность (chief executive officer),
дата(July 1), имя уходящего с должности человека (Robert L. James), имя
приходящего на должность человека (John J. Donner, Jr.).

\end{frame}

\begin{frame}
\frametitle{Weka (Waikato Environment for Knowledge Analysis)}
Weka - свободно распространяемый пакет для анализа данных,
написанный на языке Java в университете Уайкато (Новая Зеландия). \\

\vspace{1cm}
Группы алгоритмов Weka:
\begin{itemize}
\item{Классификация}
\item{Кластеризация}
\item{Задача выборки атрибутов}
\item{Задача извлечения правил ассоциаций}
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Объекты и атрибуты}
Нередко данные  об объекте хранятся в виде фиксированного списка атрибутов.\\
\vspace{1cm}
Опишем базу данных, которую мы будем использовать на следующих слайдах в качестве примера.\\
\vspace{1cm}
Возьмем в качестве рассматриваемых объектов разных животных.
Список атрибутов пусть будет следующим:
вид(строка), средний вес(число), кормление молоком(булевый), наличие шерсти(булевый), длина названия вида(число).
\end{frame}
\begin{frame}
\frametitle{Извлечение правил ассоциаций}
Основной задачей является поиск интересных и полезных закономерностей в базе данных.\\
\vspace{1cm}
\bf{Пример.}
Выберем два атрибута: средний вес и наличие шерсти.
Можем получить следующее логическое выражение: 
если средний вес = 50 кг и шерсть = 1  $\to$ вид = волк или вид = шимпанзе.
И так далее.

\end{frame}
\begin{frame}
\frametitle{Выборка атрибутов}
Задача сортировки атрибутов объектов по их информационной содержательности.\\
\vspace{1cm}
\bf{Пример.}
\vspace{1cm}
В нашей базе данных в результате действия алгоритмов выборки атрибутов получим отсортированный список: вид, средний вес, кормление молоком, наличие шерсти.\\
Очевидно, что атрибут «длина названия вида» не несет никакой информационной ценности.
\end{frame}

\begin{frame}
\frametitle{Критика идеи построения тезауруса и проекта Weka}

Сложности при работе с Weka:
\begin{enumerate}
\item{Необходимость построения БД с объектами и атрибутами}
\item{Большое число атрибутов в подобной БД}
\end{enumerate}


Алгоритм построения тезауруса основан на классификации и кластеризации.
Сами по себе алгоритмы классификации и кластеризации являюся эвристическими,
из чего следует, что в результате мы лишь приближаемся к поставленной цели, 
а так как они достаточно хорошо изучены, то каких-либо серьезных изменений ожидать не стоит.

\end{frame}

\begin{frame}
\frametitle{Граф знаний}
Граф знаний есть ориентированный(в большинстве случаев) граф, в котором вершины представляют так называемые концепты (токены и типы),
а ребра - отношения между ними, причем отношения могут быть как бинарные, так и многомерные.\\ 

\textsl{Токену} соответствует какой-либо реальный объект или абстрактное понятие, на графе он обозначается как $\Box$. \\

Токен, отражающий некоторое общее понятие и в терминологии графа знаний называется \textsl{типом}.\\
\end{frame}


\end{document}
