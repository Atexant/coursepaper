\section*{Введение}
\addcontentsline{toc}{section}{\hspace{7mm}Введение}

Wikipedia --- это свободно распространяемая и редактируемая 
Интернет-энциклопедия, расположенная по адресу wikipedia.org \cite{wikipedia}. 
Она представляет собой набор взаимосвязанных статей, охватывающих самые 
разнообразные области человеческого знания. 
Доступность Wikipedia может оказать значительное влияние на~исследования 
в~некоторых научных областях, таких~как, например, Data Mining (DM).

Сфера искусственного интеллекта, несмотря на то, что развивается
уже на протяжении нескольких десятилетий, не может похвастаться полным
достижением поставленных перед исследователями целей. Восприятие
информации и принятие решений в таком виде, как это делает человек,
остаётся вне достижимых научных пределов. 
Тем~не~менее, интенсивное развитие получили некоторые отдельные отрасли, ранее считавшиеся
разделами искусственного интеллекта, в~том числе и DM.

``Data Mining'' на~русский язык можно перевести как ``извлечение данных'',
хотя, насколько можно судить, общепринятого перевода в~русскоязычной
академической литературе пока нет. Далее в тексте будем использовать
английский вариант названия. 
DM охватывает подходы к обработке всех видов данных, включая графические и звуковые. 
Мы же ограничимся более узкой сферой, посвящённой
обработке только текстовой информации, называемой Text Mining (TM).

Wikipedia имеет некоторые преимущества перед другими массивами больших данных, 
которые часто используются в~качестве источников информации в для~баз знаний.
Материал Wikipedia лучше структурирован, чем, например, содержимое веб-страниц.
С~одной стороны, для Wikipedia в~подавляющем большинстве случаев справедливо
правило ``одна статья --- одно понятие''. 
С~другой стороны, материал
статей содержит множество ссылок, что позволяет анализировать
взаимосвязи между понятиями. 

Для~качественного и полноценного использования материала Wikipedia
необходимо решить ряд проблем технического характера. Часть из них
напрямую связано с~колоссальным количеством текста (копия текста четырех 
миллионов статей английской Wikipedia занимает порядка 36Гб), 
для~обработки которого требуется применять инструменты, хорошо
подходящие для работы со сверхбольшими данными. Дополнительно должны быть
реализованы компоненты, обеспечивающие обработку задействованных форматов. 
Кроме того, необходимо продумать удобный способ хранения страниц, учитывающий 
возможность индексации и полнотекстового поиска. Наконец, есть ряд 
прикладных задач, связанных с~обработкой естественного языка, без решения 
которых было бы невозможно получить какой-либо выигрыш в~исследованиях.

В~настоящей работе предлагается исследование некоторых путей решения  упомянутых проблем, 
а~также реализация алгоритма оценки
 справедливости некоторого утверждения, сформулированного на~ естественном языке. 
Нет~необходимости выполнять реализацию всех
 требуемых компонентов самостоятельно. Например, для~обработки
 естественного языка использовалась библиотека CoreNLP Стэнфордского
 университета \cite{corenlp}, которая позволяет определять нормальные формы слов
 (лемматизация), а~также строить дерево для определения главных и
 зависимых слов в~предложении. 
При~этом, к~сожалению, невозможно
 обработать при помощи этой библиотеки весь массив текста из
 Wikipedia, поскольку это потребовало бы нескольких месяцев вычислений
 (из предположения, что в каждой из 4 миллионов статей в среднем по 50 предложений,
 каждое из которых будет обрабатываться в среднем за 0.5 секунд), 
поэтому предлагаемый подход содержит
 сначала фазу предварительного поиска упомянутых в запросе слов для
 отсечения заведомо нерелевантных статей. Эта процедура, в свою
 очередь, также требует некоторого исследования.
