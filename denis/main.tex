Википедия (Wikipedia) --- свободно распространяемая и редактируемая 
Интернет-энциклопедия, расположенная по адресу wikipedia.org \cite{wikipedia}. По 
Фактически, она представляет собой набор взаимосвязанных статей, затрагивающих самые 
разнообразные области человеческого знания. 

Доступность Wikipedia может оказать значительное влияние на научные
исследования в области Data Mining. (Здесь расширим за счёт
какого-нибудь материала от Валентины) Это вызвано тем, что материал
Wikipedia лучше структурирован, чем содержимое веб-страниц, которые
часто используются в качестве основы для различных баз знаний. С одной
стороны, для Wikipedia в подавляющем большинстве случаев справедливо
правило ``одна статья --- одно понятие'', сдругой стороны, материал
статей содержит множество ссылок, что позволяет анализировать
взаимосвязи между понятиями. 

Для качественного и полноценного использования материала Wikipedia
необходимо решить ряд проблем технического характера. Часть из них
напрямую связано с колоссальным количеством текста (копия текста четырех 
миллионов статей английской Wikipedia занимает порядка 36Гб), 
для обработки которого требуется применять инструменты, хорошо
подходящие для работы со сверхбольшими данными. Дополнительно должны быть
реализованы компоненты, обеспечивающие обработку задействованных форматов. Кроме того 
необходимо продумать удобный способ хранения страниц, учитывающий 
возможность индексации и полнотекстового поиска. Наконец, есть ряд 
прикладных задач, связанных с обработкой естественного языка, без решения 
которых было бы невозможно получить какой-либо выигрыш в исследованиях.

В настоящей работе предлагается исследование некоторых путей решения
 упомянутых проблем, а также реализация алгоритма оценки
 справедливости некоторого утверждения, сформулированного на
 естественном языке.  Нет необходимости выполнять реализацию всех
 требуемых компонентов самостоятельно. Например, для обработки
 естественного языка использовалась библиотека CoreNLP Стэнфордского
 университета, которая позволяет определять нормальные формы слов
 (лемматизация), а также строить дерево для определения главных и
 зависимых слов в предложении \cite{fixme}. При этом, к сожалению, невозможно
 обработать при помощи этой библиотеки весь массив текста из
 Wikipedia, поскольку это потребовало бы нескольких месяцев вычислений
 (тут какие-нибудь цифры), поэтому предлагаемый подход содержит
 сначала фазу предварительного поиска упомянутых в запросе слов для
 отсечения заведомо нерелевантных статей. Эта процедура, в свою
 очередь, также требует некоторого исследования.

Рассмотрим формат хранения материала свободной энциклопедии, как он
опубликован на основном сайте проекта \cite{download}. Текст каждой статьи проходит
кодирование на двух уровнях:

1. Кодирование с использованием специальной разметки Mediawiki \cite{wikimarkup},
позволяющей задать заголовки, ссылки и пр.

2. XML-кодирование, задающее разделение статей внутри одного общего
файла с указанием идентификатора статьи, возможного перенаправления и
пр.

\subsection{Обработка XML-кодирования}

XML --- достаточно популярный формат хранения иерархических массивов
данных, широко используемый в сфере Интернет-технологий или в мире
корпоративного ПО. На многих языках программирования доступны
различные библиотеки, выполняющие разбор XML-разметки. В основном они
делятся на две группы: SAX и DOM.

1. DOM(Document Object Model) \cite{dom} --- удобный программный интерфейс доступа 
к XML-документу, рассматривающий его как дерево, где каждому элементу 
сопоставляется узел таким образом, что если элемент $a$ вложен в элемент $b$ 
в XML-документе, то в получившемся дереве узлы, соответствующие этим элементам 
будут связаны отношением “родитель-потомок”.
Основной недостаток этого метода заключается в том, что он подразумевает хранение 
полученного дерева в оперативной памяти, что в случае с файлами такого размера 
как дампы Wikipedia недопустимо.

2. SAX (Simple API for XML) --- метод последовательного чтения XML-документа \cite{sax}. 
При его использовании обработчик последовательно читает данные
из XML-файла и по мере нахождения новых элементов разметки сообщает об этом вызвавшему приложению, 
используя клиентские функции обратного вызова[7]. Таким образом приложение-клиент 
SAX-парсера может получить полную информацию о содержимом и структуре документа,
не загружая его целиком в память. Однако и у этого метода есть свои недостатки. 
Например, если нужно получить содержимое элемента, который находится в конце файла,
придётся обработать все предшествующие элементы, что при размерах документа, сравнимых
с размерами дампа Wikipedia, может занять довольно много времени.

Учитывая достоинства SAX-парсеров, позволяющих обрабатывать объемные XML-документы, 
был выбран именно этот метод для реализации, несмотря на некоторые недостатки.
Как говорилось выше, дамп базы данных Wikipedia \cite{dump} - это заархивированный XML-файл, размер которого 
в распакованном виде составляет порядка 36Гб. Из XML-метасхемы \cite{schema} 
и при ознакомительном изучении самого дампа видно, что статьи в нем представлены в виде 
тэгов ``page" верхнего уровня. Каждый из них содержит следующие вложенные теги:

a. title --- содержит заголовок статьи, уникальный в рамках Wikipedia

b. id --- содержит уникальный идентификатор статьи, положительное целое число

с. revision --- тег содержит описание последнего изменения статьи (дату, автора), 
а так же непосредственно последнюю версию текста статьи во вложенном тэге ``text''. 
Тег ``page'', соответствующий какой-либо, может также содержать внутри себя %%тут надо поправить, смысл получается не до конца ясен.
элемент “redirect”, что означает, что статья с таким названием дублирует 
какую-то другую статью, в этом ее тег ``text" содержит заголовок дублируемой статьи. 

В рамках данной работы основными требованиями к компоненту, отвечающему 
за обработку этого файла были гибкость и необходимость доступа к произвольному участку документа.
Необходимость в последнем вызвана тем, что:

a. Произвольный доступ нужен для параллельной обработки.

b. При исключительной ситуации, если обработчик некорректно завершает свою работу и 
нет возможности установить указатель на место в файле, где это случилось,
необходимо начинать обработку всего дампа сначала, что не очень удобно.

В связи с предположением о большом количестве разнородных компонентов,
которым может понадобится доступ к дампам Wikipedia, было принято решение 
использовать следующую гибкую архитектуру XML-парсера:

1. Объект parser класса WikipediaParser запускает SAX-парсер, который “сообщает” 
ему о встреченных XML-элементах и их содержимом.

2. Предварительно этот объект конфигурируется другим объектом - обработчиком статей,
 реализующий интерфейс WikipediaPageHandler, содержащий метод handle(WikipediaPage).

3. parser при прочтении целиком тега page создает объект WikipediaPage, заполняет 
необходимые поля и передает его обработчику статей.

%%Тут можно поместить локальную диаграмму классов или еще чего-нибудь такое. А может и не нужно 
%% Ну вообще можно, было бы неплохо.

Таким образом была получена универсальная система чтения дампа, для использования которой 
достаточно реализовать простой интерфейс с обработкой одного объекта-страницы.

Для решения проблемы произвольного доступа к файлу была создана обертка для стандартного потока InputStream, которая:

1. Предоставляет возможность определения сдвига в байтах от начала файла XML-документа

2. Если заданный сдвиг не указывает на начало тега page, пропустить все байты, 
вплоть до следующего его упоминания.

3. Самостоятельно “обернутьпредоставляемые потоком данные в корневой тег.

Последние два пункта нужны для того, чтобы содержимое сформированного потока являлось бы допустимым XML-кодом.
В результате была реализована возможность обрабатывать файл-дамп с произвольным
сдвигом от его начала, что значительно увеличило удобство использования этого компонента.  

\subsection{Разметка Mediawiki}

Текст статей Wikipedia включает в себя кроме всего прочего элементы разметки[8], которые позволяют:

1. Управлять внешним видом блоков текста (размер/тип шрифта, цвет, отступ и т.п.)

2. Создавать разного рода специфичные структуры оформления информации (таблицы, вложенные списки, оглавления)

3. Выделять некоторые части текста, как ссылки на другие стать.и

Поскольку разметка содержит одновременно элементы форматирования внешнего вида,
несущественные для интерпретации текста, и элементы, значимые при обработке,
необходим компонент, способный выполнять фильтрацию ненужных деталей форматирования
и передающий обработанные данные в абстрактный унифицированный интерфейс по аналогии с тем, как это делалось при разборе XML.
В качестве решения этой проблемы было предложено использовать библиотеку 
JWPL (Java Wikipedia Library)[9], которая содержит инструменты по работе с разметкой 
Mediawiki[8] и дает программный доступ к отдельным ее элементам, таким как:
a. Список ссылок на другие статьи, встречающиеся в тексте этой статьи
b. Вложенные списки и оглавления, представленные в виде древообразных структур
c. Таблицы из конкретной статьи и удобный интерфейс для их обработки

Для того, чтобы обеспечить независимость от этой библиотеки, в соответствии
с паттерном Шлюз[15] был создан класс, предоставляющий остальным компонентам системы
доступ к этой библиотеке через свои методы

\subsection{Хранение в базе данных}

В качестве базы данных для хранения страниц изначально была выбрана свободная реляционная 
СУБД MySQL[11], представляющая собой удобный инструмент для решения поставленных 
задач с возможностью индексирования и хорошей масштабируемостью, однако не с самыми
лучшими показателями в плане полнотекстого поиска. По причине последнего недостатка было 
решено ограничиться хранением и индексированием в MySQL следующей информации о страницах:

1. Числовой идентификатор

2. Заголовок

3. Если страница является перенаправлением, заголовок и идентификатор  базовой статьи.

4. Сдвиг в байтах от начала дампа-исходника, где расположена данная страница

То есть для того, чтобы получить текст определенной страницы, например по названию,
нужно найти строку в таблице статей с фильтром по названию,
получить сдвиг для данной статьи, и, воспользовавшись
описанным выше компонентом для парсинга дампа, найти интересующую вас статью.

Для более удобной работы с БД, используя паттерн проектирования Data Mapper[14], 
был создан компонент для сохранения и поиска страниц Wikipedia в базе.

В ходе дальнейших исследований предполагается изучение более производительных БД, таких
как Lucene \cite{lucene}, MongoDB \cite{mongoDB}, лучше приспособленных для поиска в
массивах текстов подобного размера.

\subsection{Natural Language Processing}

Обработка естественного языка (Natural Language Processing, NLP)\cite{textminingsurvey} - одна из самых 
важных областей для исследований, сопутствующих извлечению информации из текстов.
Основная цель NLP состоит в улучшении понимания естественных языков компьютерами.
Из базовых ее направлений стоит упомянуть следующие.

1. Морфологический анализ текста, суть которого чаще всего сводится к определению
частей речи для отдельных слов текста.
2. Лемматизация - приведение слов текста к нормальному виду (инфинитив, ед. число, и т.д.)
3. Синтаксический анализ - нахождение связей между членами предложений в тексте.

Задачи, связанные с каждым из вышеперечисленных аспектов NLP, 
являются предметом отдельных научных исследований и выходят за рамки текущей научной работы.
В значительной мере они были проведены исследователями Стэнфордского университета. 
Для решения проблем, связанных с обработкой естественных языков, был использована библиотека  Stanford CoreNLP,
реализующие имеющиеся достижения.

\subsubsection{Stanford CoreNLP}
Пакет Stanford CoreNLP\cite{corenlp} - один из самых развитых инструментов по работе 
с естественными языками, первая версия которого датируется 1 ноябрем 2010 года.
По сути он является набором классов на языке Java, предоставляющих интерфейсы
для решения многочисленных проблем NLP. В данной работе использовались две подсистемы CoreNLP:
POS-tagger, предназначенный для определения частей речи слов и их лемматизации,
и Parser, предоставляющий возможность строить дерево зависимостей членов предложений.

Основными понятиями пакета CoreNLP являются Аннотации и Аннотаторы. 
Для того, чтобы получить те или иные результаты работы NLP-алгоритмов,
пользователь пакета должен создать объект-документ,
содержащий коллекцию аннотаций для исходного текста. 
Затем необходимо выбрать Аннотаторы, каждый из которых решает свою задачу, 
добавляя в коллекцию соответсвующие аннотации, и применить их к тексту. 
Нужно учитывать, что некоторые аннотаторы требуют результаты работы других,
например аннотатор, ответственный за лемматизацию, может быть запущен
только в том случае, если документ уже был обработан аннотатором частей речи.

Некоторые аннотаторы позволяют разбивать текст на отдельные предложения и слова,
причем аннотации, которые были применены к исходному документу, будут присутствовать и
и в выделенных его частях. 
То есть если применить к тексту аннонаторы разбивающие текст на отдельные слова
вместе с аннотатором частей речи, тогда результаты работы второго аннотатора
можно получить как для всего текста, так и для отдельных его слов.

Список всех аннотаторов и их аннотаций можно найти на сайте CoreNLP\cite{corenlp}.

\subsubsection{Способы представления лингвистической информации в CoreNLP}

Как было сказано выше, в данной работе CoreNLP использовался в основном для 
определения частей речи и анализа грамматических зависимостей
членов предложений.
Если с морфологическим анализом все относительно просто - каждому слову
из текста сопоставляется один из тегов частей речи, список которых можно найти в \cite{treebank},
то с синтаксической частью все чуть менее прозрачно.

Базовым способом представления результатов этой части является структура,
которая в лингвистике имеет название грамматика непосредственно составляющих(phrase structure grammar), 
описание формата которой можно найти в книге Хомского "Синтаксические структуры"\cite{homsky}.
Это представление является наиболее полным и обоснованным с точки зрения лингвистики, 
однако не очень удобным для анализа связей между отдельными членами предложения.
Для решения этой проблемы в пакете CoreNLP существуют инструменты для 
преобразования грамматики непосредственно составляющих к более простой форме
списка зависимостей членов предложения, каждый элемент которого имеет вид 
<тип зависимости>(<слово1>-<номер слова1>,<слово2>-<номер слова2>).
Причем первое слово в паре считается главным, а второе - зависимым.
Список типов зависимостей можно найти в \cite{dependencies}.

Если построить граф с вершинами-членами предложений, а в качестве ребер использовать 
отдельные зависимости из полученного списка, то этот граф, в силу того, что у каждого члена
предложения может быть не более одного управляющего слова, очевидно будет
являться деревом, которое далее в тексте будет называться деревом зависимостей предложения.
Именно эта форма была выбрана для дальнейшей работы со структурой
предложения.

%%может стоит привести пример результата работы всего этого?
%% Да, пожалуй, что со стороны может быть не сразу ясно. Пример очень уместен.

\subsubsection{Ограничения CoreNLP}

Алгоритмы библиотеки CoreNLP имеют ряд существенных ограничений, 
о которых не стоит забывать, рассматривая его как альтернативу
при выборе NLP-пакета.

Одной из проблем может стать тот факт, что CoreNLP в его текущей комплектации
предназначен в основном для работы с английским языком, 
и его адаптация к лингвистическим особенностям других языков, включая русский,
является нетривиальной задачей, если вообще решаемой.

Вторым аспектом, ограничивающим применение CoreNLP является трудоемкость
его алгоритмов. При загрузке всех своих словарей, необходимых для морфологического
анализа, структуры CoreNLP могут занять до одного гигабайта оперативной памяти.
Кроме того, стоит сказать, что время работы парсера, строящего дерево предложения,
так же оставляет желать лучшего: измерения показали, что для предложения
из обычного художественного текста, среднее значение времени работы 0.5 секунд,
а в случае сложных предложений может доходить и до одной секунды.

\subsection{Оценка близости предложений}
Оценивание похожести предложений является на сегодня одной из самых важных
частей алгоритмов связанных с классификацией и кластеризацией документов, а также
с определением оригинальности авторства текстов. Кроме всего прочего модифицированные методы определения
близости можно использовать и при поиске по набору документов.

Сформулируем задачу следующим образом:
введём меру близости как функцию от двух аргументов-предложений, отвечающую следующим требованиям:

1. Ее значения находятся в интервале [0, 1].

2. Если взять значения этой меры для двух различных пар предложений, 
то большее значение должно соответствовать той паре, которая по мнению
большинства опрошенных людей будет считаться более близкой по смыслу.

По причине слабой формализации второй части определения все алгоритмы 
построения этой меры носят эвристический характер и не могут быть строго научно сформулированы.

В рамках данной научной работы был исследован ряд уже существующих алгоритмов,
и один из них, рассмотренный как наиболее перспективный, был реализован.
Кроме того были оценены результаты его работы, в том числе на примере статьи Wikipedia, 
а также выдвинуты предложения по его улучшению.

Далее рассмотренные методы можно разделить на следующие три типа:

1. Алгоритмы первого типа\cite{statisticalSim} рассматривают предложения и отдельные слова только как символьные строки, 
определяя меру похожести на основе разнообразных статистических методов, руководствуясь только сходством в написании слов.
2. Второй класс методов\cite{wordnetSim} рассматривает предложение, как набор понятий, позволяя распознавать синонимы 
или слова, находящиеся в разных формах, как одно и то же, позволяя определить сходство предложений, 
составленных из разных слов с одним смыслом.
3. И наконец третий тип алгоритмов\cite{weightedDep}\cite{complexSim}, который кроме смыслового значения слов для определения похожести
использует так же и связи, которые образуют эти слова в предложении.

\subsubsection{Обзор существующих алгоритмов}

Авторы статьи Calculating Statistical Similarity between Sentences\cite{statisticalSim} сравнивают предложения, 
как упорядоченные списки составляющих их слов, причем слова сравниваются только исходя из эквивалентности их написания. 
Для установления степени близости таких двух списков авторы предлагают несколько статистических методов,
начиная от самых стандартных, таких как определение отношения пересечения множества слов в этих списках к их объединению
или представление этих списков, как векторы, для которых похожесть определяется, как косинус угла между ними,
до более сложных формул, учитывающих порядок слов в предложении и даже расстояния между ними.

Преимущество такого подхода состоит в том, что он прост в реализации, не требует
сложных дополнительных инструментов, обладает низкой трудоемкостью, и кроме всего
прочего полученная реализация не будет зависеть от языка предложений, 
так как использует только написание слов.
Основным недостатком методов этого типа является то, что они совсем
не приспособлены к многозначности и полиморфизму естественных языков, и,
например, в случае когда два предложения имеют схожий смысл, но используют разные по написанию слова,
значение меры сходства будет близким к нулю.

Для борьбы с этими недостатками можно воспользоваться методами, описанные в статье Dao, T.N. & Simpson, T.\cite{wordnetSim}.
Ее авторы предлагают использовать WordNet\cite{wordnet} - семантическую сеть для английского языка, 
разработанную в Принстонском университете - для оценки похожести отдельных слов-членов предложения.
Общая суть работы алгоритма состоит в том, что для исходных предложений
строится полный взвешенный двудольный граф, вершинами которого являются отдельные
слова каждого из предложений, а вес каждого из ребер равен значению меры похожести слов,
соответсвующих этим вершинам. А результатом работы алгоритма является максимальное
взвешенное паросочетание на этом графе, находимое венгерским алгоритмом.
Для того, чтобы мера похожести принадлежала интервалу [0,1] ее нормируют на вес ребер.

Таким образом, находится такое соответствие между словами двух предложений,
что оно обеспечивает максимальную суммарную близость.

Подобные алгоритмы, сравнивающие не просто близость написания слов, 
но и близость их смысла, могут дать более точные результаты в случаях, 
когда близость предложений скрыта синонимичными различиями в написании.
Однако для их реализации необходимы такие наработки из областей NLP и Data Mining, 
как лемматизация слов и семантическая сеть, подобная WordNet\cite{wordnet}.

%% тут можно опять же пересечь с материалом Валентины, поскольку у неё последний разбор был про мегаразвитую сеть понятий.

Кроме того такие алгоритмы не учитывают роли, которые играют слова в предложении,
а так же их отношения, что завышает итоговую меру похожести в случаях,
когда сравниваются предложения со схожими наборами слов, но разным смыслом,
заключенным в синтаксической структуре.

В свою очередь третий тип методов призван решить последнюю проблему. 
Алгоритмы этого класса устанавливают меру похожести предложений на основании двух факторов:

1. Схожесть семантических ролей отдельных членов.

2. Аналогичность синтаксических отношений, образующих эти предложения.

Для установления величины второго фактора в этих алгоритмах чаще всего используются
деревья зависимостей предложений, суть которых изложена выше в главе "Способы представления лингвистической информации в CoreNLP".
Автором были найдены две статьи, описывающие методы, 
которые используют в своих оценках связи между отдельными членами предложений.

В статье Paraphrase Identification Using Weighted Dependencies and Word Semantics\cite{weightedDep}
предлагается следующий подход:
1. Строятся деревья зависимостей оцениваемых предложений.

2. Затем каждому ребру присвается вес, как величина прямо зависимая

от расстояния до корня. Эта величина характеризует "значимость" конкретной зависимости.
Предполагается, что чем дальше от корня находятся слова, тем меньше они влияют
на общий смысл предложения. Кроме того в этом весе учитывается тип конкретной связи.
Так например связь-"управление" между глаголом и существительным
может быть оценена выше, чем "согласование" между именами существительным и прилагательным.

3. Далее вводится мера похожестей отдельных связей, как выпуклая оболочка 
величин схожести управляющих и подчиненных слов в них.

4. Наивным алгоритмом находится максимальное паросочетание на графе,
построенном по аналогии с тем, как это сделано в статье \cite{wordnetSim}.

5. Зависимости, для которых не были найдены пары, либо значения похожести
найденной пары в максимальном паросочетании меньше определенного порога,
помечаются как беспарные.

6. В качестве меры похожести предлагается величина 
Paraphrase(S1, S2) = Sim(S1,S2)/Diss(S1, S2), 
где Sim(s1, s2) - вес максимального паросочетания,
а Diss(S1, S2) величина, прямо пропоционально зависящая от 
суммы весов "значимости" для беспарных зависимостей.

Предложенный алгоритм учитывает как семантическую, так и синтаксическую составляющую
предложений, что потенциально позволяет всесторонне оценить искомую меру близости.
Кроме того авторы статьи предлагают достаточно интересную идею оценивания значимости связей
в зависимости от их расстояния до корня дерева и  их типа, такой подход кажется достаточно очевидным
и эмпирически обоснованным, так что факт того, что этот аспект совсем не учитывается в алгоритме,
описанном в следующей рассмотренной статье, можно считать значительным недоработкой.

В ходе анализа эффективности алгоритма автором был выявлен ряд недостатков,
и в связи с этим было принято решение взять для реализации не этот, а описанный ниже алгоритм.
Во-первых, следует учесть то, что авторами статьи изначально были поставлены несколько иные
цели. Для них главным был ответ на вопрос, можно ли считать одно из предложений парафразом,
то есть пересказом, другого. А это уже несколько более узкая задача,
результаты которой не всегда могут коррелировать с похожестью предложений.
Вторым выявленным недостатком этого метода является примитивность
используемого математического аппарата. В частности можно
упомянуть несостоятельность выбора алгоритма паросочетания, 
предложенный авторами вариант часто может находить неоптимальные варианты,
что уменьшает эффективность метода в целом. Кроме того для
подсчета итогового коэффициента вероятности того, что данные предложения
являются парафразами, использовались достаточно наивные статистические методы,
которые могут давать не слишком точные результаты.

Авторы статьи A Dependency Grammar and WordNet Based Sentence Similarity Measure\cite{complexSim}
предлагают более комплексный подход, суть которого в целом заключается в декомпозиции задачи
оценивания близости предложений на подзадачи оценивания более частных аспектов, 
характеризующих их близость, и суммирования затем полученных величин.
В качестве варианта декомпозиции предлагается разбить итоговую меру близости, 
на оценивание семантической и синтаксической близости данных предложений отдельно.
В силу их независимости и сложности, дальнейшее разбиение
каждой из этих частей описано в отдельных разделах ниже.

\subsubsubsection{Семантическая близость предложений}
Несмотря на то, что синтаксическая схожесть рассматривается отдельно, 
при оценке семантической части используются не отдельные слова, а
вершины дерева зависимостей вместе с их непосредственными потомками.

Авторами предлагается сравнивать отдельные вершины, суммируя схожесть их главных слов
и схожесть наборов слов-потомков.
Для определения близости наборов-слов D1,D2 используется формула, согласно которой
величина близости равна среднему значению для каждого слова d1i из D1 максимальной схожести
со словом d2j из D2. 
Семантическая близость отдельных слов определяется по тому же методу, что и в \cite{wordnetSim}.

Определившись с мерой близости вершин, авторы статьи предлагают 
алгоритм вычисления семантической близости, изложенный далее по тексту.

В начале для каждого из предложений строится по два множества вершин:
в первое попадают только те, у которых главное слово является именем существительным,
а во второе только вершины-глаголы.

Величина семантической близости полагается равной

Simsem (S1 , S2 ) = β1 × Sims(v1, v2 ) + β2 × Sims(n1, n2), где 
β1 + β2 = 1, 
v1, v2, n1, n2 - множества вершин-глаголов и существительных для каждого из предложений,
а Sims - оценка близости множеств-вершин, описанная ниже.

Для сравниваемых множеств вершин A1,A2 применяется стандартный для таких случаев подход,
подразумевающий введение векторного пространства, базисом которого является объединение этих множеств,
и размерностью соответственно равной |A1|+|A2|.  
Каждому из входных множеств A1,A2 ставится в соответствие вектора a1,a2.

Причем l-ая компонента вектора полагается равной 
ai,l = max(Simn(Nl, Aij)) 
для i=1,2; j=1,|Ai|,
где Nl - вершина-l-ый элемент базиса введенного векторного пространства,
а Simn - мера схожести двух вершин, определение которой можно найти выше.

После определения векторов, похожесть множеств вершин A1,A2 Sims(A1,A2) определяется
как косинус угла между соответственными векторами. 
Это позволяет оценить величину угла отклонения одного множества от другого в нормальной шкале
со значениями [0,1], и как следствие силу различий между этими множествами.

Полученная таким образом величина семантической близости двух предложений
учитывает сходство употребленных в них глаголов и существительных,
как наиболее информацонно-содержательных частей речи, 
причем стоит отметить, что учитывается не просто схожесть 
отдельных, лишенных контекста слов, но и близость их 
подчинненых членов. После ее вычисления авторами предлагается оценить
синтаксическую близость данных предложений.

\subsubsubsection{Синтаксическая близость предложений}
Для подсчета этой величины авторы используют в качестве входных данных
модифицированные деревья зависимостей, в которых для вершин известно только то, 
какой части речи принадлежало соответсвующее им слово.
А ребра так же, как и в обычных деревьях зависимостей, отражают
наличие и тип связи между словами.
Таким образом в частном случае такое дерево может быть задано
набором ребер-триплетов вида:
(часть речи управляющего слова, часть речи подчиненного слова, тип связи).

В качестве искомого значения синтаксической схожести предлагается определить меру идентичности
полученных деревьев. В частности в описанном в статье алгоритме
используется формула, для которой деревья представляются в упрощенном виде
множест триплетов Ssyn1 и Ssyn2, формат которых описан выше.
Формула имеет вид Simsyn(S1, S2) = 2 x |Ssyn1 U Ssyn2| / (|Ssyn1| + |Ssyn2|), 
то есть значение прямо пропорциональное количеству одинаковых ребер
и обратно пропорциональное суммарному их числу.


Как результат оценивания суммарной меры близости предложений, 
дается следующая формула:
Sim(S1, S2) = alpha1 * Simsem(S1, S2) + alpha2*Simsym(S1, S2), при alpha1+alpha2=1,
а Simsem(S1, S2) и Simsym(S1, S2) величины, вычисление значений которых
описано выше.

Благодаря такому подходу, предусматривающему декомпозицию при установлении результата, 
можно оценивать каждый из параметров, влияющих на мнение человека о близости
смысла предложений, отдельно. Авторами предложен ряд таких параметров, 
которые достаточно детально и всесторонее исследуют вопрос о схожести. Кроме того
в ходе построения алгоритма создается каркас для добавления дополнительных параметров,
позволяющих более достоверно оценить искомую величину. 
В указанной статье также есть приложение, демонстрирующее качество работы
полученного алгоритма на примере оцененного группой людей набора пар предложений.
Авторы сравнивают полученные алгоритмом меры близости данных пар с оценками людей, 
а так же с результатами других методов, что позволяет сделать практический вывод
о состоятельности подхода.

Автором настоящей работы было принято решение взять для реализации именно этот метод,
как наиболее перспективный из всех рассмотренных, и кроме того обладающий
потенциалом для его дальнейшего улучшения.

В процессе анализа, реализации и оценки результатов работы были выявлены
некоторые недостатки этого подхода.
В ряду самых критичных из них следует упомянуть его время работы - 
по причине того, что синтаксический анализ является одной из самых
сложных и трудоемких задач NLP, и при этом самой важной частью алгоритма,
суммарное время работы может доходить до нескольких секунд,
что неприемлимо при поиске по большим базам данных.
Кроме того, из-за того, что в предыдущем методе, описанном в \cite{weightedDep},
так же используются результаты синтаксического парсера, можно сделать вывод
о наличии у него такого же рода недостатка.
Еще одним недостатком данного алгоритма можно считать то, что в нем совсем
не учитывается близость вершин к корню дерева зависимостей, хотя этот аспект
имеет большое значение при оценке влияния конкретных слов на общий смысл
предложения.
Также стоит обратить внимание на то, что в алгоритме
используется достаточно простой способ определения величины изоморфности
синтаксических деревьев, который использует только набор ребер, но не то,
как они относительно друг друга расположены, что снижает точность
результатов этой части в случаях больших предложений со сложной структурой.

\subsection{Реализация алгоритма комплексного анализа близости предложений}
В рамках настоящей работы автором был реализован алгоритм оценивания
схожести предложений, который описан в статье\cite{complexSim} и
проанализирован в предыдущем разделе.
Для реализации был выбран язык программирования Java и библиотека CoreNLP\cite{corenlp},
используящаяся для определения частей речи данных предложений, а так же для выявления зависимостей
между членами предложений.

Для того, чтобы обеспечить большую гибкость и обеспечить независимость от внешних библиотек,
которые может потребоваться заменить в случае обнаружения более эффективных разработок,
были созданы интерфейсы шлюзов доступа к ним и их конкретные реализации, напрямую
использующие сервисы, предоставляемые этими библиотеками.
По причине удобства представления предложения в виде дерева его связей
необходимо было также реализовать алгоритм его построения из списка ребер, 
вид которых описан "Способы представления лингвистической информации в CoreNLP".
Для этого был создан класс SentenceTreeBuilder и в ходе работы расширен
классом SentenceTreeWithWordTokensBuilder, в зону ответственности которого
кроме построения дерева входит также определение частей речи и приведение к нормальной форме
его вершин.

%%Здесь будет UML-диаграмма, изображающая отношения между классами
%% Sentence, SentenceTree, SentenceTreeBuilder, SentenceTreeWithWordTokensBuilder, CoreNLPAccess
