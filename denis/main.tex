Википедия (Wikipedia) --- свободно распространяемая и редактируемая 
Интернет-энциклопедия, расположенная по адресу wikipedia.org \cite{wikipedia}. По 
Фактически, она представляет собой набор взаимосвязанных статей, затрагивающих самые 
разнообразные области человеческого знания. 

Доступность Wikipedia может оказать значительное влияние на научные
исследования в области Data Mining. (Здесь расширим за счёт
какого-нибудь материала от Валентины) Это вызвано тем, что материал
Wikipedia лучше структурирован, чем содержимое веб-страниц, которые
часто используются в качестве основы для различных баз знаний. С одной
стороны, для Wikipedia в подавляющем большинстве случаев справедливо
правило ``одна статья --- одно понятие'', сдругой стороны, материал
статей содержит множество ссылок, что позволяет анализировать
взаимосвязи между понятиями. 

Для качественного и полноценного использования материала Wikipedia
необходимо решить ряд проблем технического характера. Часть из них
напрямую связано с колоссальным количеством текста (копия текста четырех 
миллионов статей английской Wikipedia занимает порядка 36Гб), 
для обработки которого требуется применять инструменты, хорошо
подходящие для работы со сверхбольшими данными. Дополнительно должны быть
реализованы компоненты, обеспечивающие обработку задействованных форматов. Кроме того 
необходимо продумать удобный способ хранения страниц, учитывающий 
возможность индексации и полнотекстового поиска. Наконец, есть ряд 
прикладных задач, связанных с обработкой естественного языка, без решения 
которых было бы невозможно получить какой-либо выигрыш в исследованиях.

В настоящей работе предлагается исследование некоторых путей решения
 упомянутых проблем, а также реализация алгоритма оценки
 справедливости некоторого утверждения, сформулированного на
 естественном языке.  Нет необходимости выполнять реализацию всех
 требуемых компонентов самостоятельно. Например, для обработки
 естественного языка использовалась библиотека CoreNLP Стэнфордского
 университета, которая позволяет определять нормальные формы слов
 (лемматизация), а также строить дерево для определения главных и
 зависимых слов в предложении \cite{fixme}. При этом, к сожалению, невозможно
 обработать при помощи этой библиотеки весь массив текста из
 Wikipedia, поскольку это потребовало бы нескольких месяцев вычислений
 (тут какие-нибудь цифры), поэтому предлагаемый подход содержит
 сначала фазу предварительного поиска упомянутых в запросе слов для
 отсечения заведомо нерелевантных статей. Эта процедура, в свою
 очередь, также требует некоторого исследования.

Рассмотрим формат хранения материала свободной энциклопедии, как он
опубликован на основном сайте проекта \cite{download}. Текст каждой статьи проходит
кодирование на двух уровнях:

1. Кодирование с использованием специальной разметки Mediawiki \cite{wikimarkup},
позволяющей задать заголовки, ссылки и пр.

2. XML-кодирование, задающее разделение статей внутри одного общего
файла с указанием идентификатора статьи, возможного перенаправления и
пр.

\subsection{Обработка XML-кодирования}

XML --- достаточно популярный формат хранения иерархических массивов
данных, широко используемый в сфере Интернет-технологий или в мире
корпоративного ПО. На многих языках программирования доступны
различные библиотеки, выполняющие разбор XML-разметки. В основном они
делятся на две группы: SAX и DOM.

1. DOM(Document Object Model) \cite{dom} --- удобный программный интерфейс доступа 
к XML-документу, рассматривающий его как дерево, где каждому элементу 
сопоставляется узел таким образом, что если элемент $a$ вложен в элемент $b$ 
в XML-документе, то в получившемся дереве узлы, соответствующие этим элементам 
будут связаны отношением “родитель-потомок”.
Основной недостаток этого метода заключается в том, что он подразумевает хранение 
полученного дерева в оперативной памяти, что в случае с файлами такого размера 
как дампы Wikipedia недопустимо.

2. SAX (Simple API for XML) --- метод последовательного чтения XML-документа \cite{sax}. 
При его использовании обработчик последовательно читает данные
из XML-файла и по мере нахождения новых элементов разметки сообщает об этом вызвавшему приложению, 
используя клиентские функции обратного вызова[7]. Таким образом приложение-клиент 
SAX-парсера может получить полную информацию о содержимом и структуре документа,
не загружая его целиком в память. Однако и у этого метода есть свои недостатки. 
Например, если нужно получить содержимое элемента, который находится в конце файла,
придётся обработать все предшествующие элементы, что при размерах документа, сравнимых
с размерами дампа Wikipedia, может занять довольно много времени.

Учитывая достоинства SAX-парсеров, позволяющего обрабатывать объемные XML-документы, 
для реализации был выбран именно этот метод.
Как говорилось выше, дамп базы данных Wikipedia \cite{dump} - это заархивированный XML-файл, размер которого 
в распакованном виде составляет порядка 36Гб. Из XML-метасхемы \cite{schema} 
и при поверхностном изучении самого дампа видно, что статьи в нем представлены в виде 
тэгов ``page" верхнего уровня. Каждый из них содержит следующие вложенные теги:

a. title --- содержит заголовок статьи, уникальный в рамках Wikipedia

b. id --- содержит уникальный идентификатор статьи, положительное целое число

с. revision --- тег содержит описание последнего изменения статьи (дату, автора), 
а так же непосредственно последнюю версию текста статьи во вложенном тэге ``text''. 
Тег ``page'', соответствующий какой-либо, может также содержать внутри себя %%тут надо поправить, смысл получается не до конца ясен.
элемент “redirect”, что означает, что статья с таким названием дублирует 
какую-то другую статью, в этом ее тег ``text" содержит заголовок дублируемой статьи. 

В рамках данной работы основными требованиями к компоненту, отвечающему 
за обработку этого файла были гибкость и необходимость доступа к произвольному участку документа.
Необходимость в последнем вызвана тем, что:

a. Произвольный доступ нужен для параллельной обработки.

b. При исключительной ситуации, если обработчик некорректно завершает свою работу и 
нет возможности установить указатель на место в файле, где это случилось,
необходимо начинать обработку всего дампа сначала, что не очень удобно.

В связи с предположением о большом количестве разнородных компонентов,
которым может понадобится доступ к дампам Wikipedia, было принято решение 
использовать следующую гибкую архитектуру XML-парсера:

1. Объект parser класса WikipediaParser запускает SAX-парсер, который “сообщает” 
ему о встреченных XML-элементах и их содержимом.

2. Предварительно этот объект конфигурируется другим объектом - обработчиком статей,
 реализующий интерфейс WikipediaPageHandler, содержащий метод handle(WikipediaPage).

3. parser при прочтении целиком тега page создает объект WikipediaPage, заполняет 
необходимые поля и передает его обработчику статей.

%%Тут можно поместить локальную диаграмму классов или еще чего-нибудь такое. А может и не нужно 
%% Ну вообще можно, было бы неплохо.

Таким образом была достигнута гибкая система обработки дампа, для использования которой 
достаточно реализовать простой интерфейс с обработкой одного объекта-страницы.

Для решения проблемы произвольного доступа к файлу была создана обертка для стандартного потока InputStream, которая:
1. Обладает возможностью задания сдвига в байтах от начала файла с XML-документом
2. Если заданный сдвиг не указывает на начало тега page, пропустить все байты вплоть 
до начала следующего такого тега
3. Искусственно “обернуть” отдаваемые потоком данные в корневой тег.
Последние два пункта нужны для того, чтобы содержимое получившегося потока соответсвовало формату XML

В результате была реализована возможность обрабатывать файл-дамп с произвольным
сдвигом от его начала, что значительно увеличило удобство использования этого компонента.  

[ЗАГОЛОВОК Разметка Mediawiki]

Текст статей Wikipedia включает в себя кроме всего прочего элементы разметки[8], которые позволяют:
1. Управлять внешним видом блоков текста (размер/тип шрифта, цвет, отступ и т.п.)
2. Создавать разного рода специфичные структуры оформления информации (таблицы, вложенные списки, оглавления)
3. Выделять некоторые части текста, как ссылки на другие статьи

Так как разметка статей содержит в себе важные, информационно-значимые части, 
необходимо было разработать систему, очищающую текст от ее частей, связанных с оформлением,
и, одновременно предоставляющую удобный интерфейс доступа к информативным элементам.

В качестве решения этой проблемы было предложено использовать библиотеку 
JWPL (Java Wikipedia Library)[9], которая содержит инструменты по работе с разметкой 
Mediawiki[8] и дает программный доступ к отдельным ее элементам, таким как:
a. Список ссылок на другие статьи, встречающиеся в тексте этой статьи
b. Вложенные списки и оглавления, представленные в виде древообразных структур
c. Таблицы из конкретной статьи и удобный интерфейс для их обработки

Для того, чтобы обеспечить независимость от этой библиотеки, в соответствии
с паттерном Шлюз[15] был создан класс, предоставляющий остальным компонентам системы
доступ к этой библиотеке через свои методы

[ЗАГОЛОВОК Хранение в базе данных]

В качестве базы данных для хранения страниц изначально была выбрана свободная реляционная 
СУБД MySQL[11], представляющая собой удобный инструмент для решения поставленных 
задач с возможностью индексирования и хорошей масштабируемостью, однако не с самыми
лучшими показателями в плане полнотекстого поиска. В свете последнего недостатка было 
решено хранить и индексировать в MySQL следующую информацию о страницах:

1. Числовой идентификатор

2. Заголовок

3. Если страница является перенаправлением, заголовок и идентификатор  базовой статьи.

4. Сдвиг в байтах от начала дампа-исходника, где расположена данная страница

То есть для того, чтобы получить текст определенной страницы, например по названию,
нужно найти строку в таблице статей с фильтром по названию,
получить сдвиг для данной статьи, и, воспользовавшись
описанным выше компонентом для парсинга дампа, найти интересующую вас статью.

Для более удобной работы с БД, используя паттерн проектирования Data Mapper[14], 
был создан компонент для сохранения и поиска страниц Wikipedia в базе.

В ходе дальнейших исследований предполагается изучение более производительных БД, таких
как Lucene \cite{lucene}, MongoDB \cite{mongoDB}, лучше приспособленных для поиска в
массивах текстов подобного размера.

[ЗАГОЛОВОК Natural Language Processing]

Обработка естественного языка (Natural Language Processing, NLP)\cite{textminingsurvey} - одна из самых 
важных областей для исследований, сопутствующих извлечению информации из текстов.
Основная цель NLP состоит в улучшении понимания естественных языков компьютерами.
Из базовых ее направлений стоит упомянуть следующие.

1. Морфологический анализ текста, суть которого чаще всего сводится к определению
частей речи для отдельных слов текста.
2. Лемматизация - приведение слов текста к нормальному виду (инфинитив, ед. число, и т.д.)
3. Синтаксический анализ - нахождение связей между членами предложений в тексте.

Задачи, связанные с каждым из вышеперечисленных аспектов NLP, 
являются поводом для отдельных научных исследований и не входят в рамки данной научной работы.
В связи с этим для решения проблем, связанных с обработкой естественных языков, был использован Stanford CoreNLP.

[ЗАГОЛОВОК Stanford CoreNLP]
Пакет Stanford CoreNLP\cite{corenlp} - один из самых развитых инструментов по работе 
с естественными языками, первая версия которого датируется 1 ноябрем 2010 года.
По сути он является набором классов на языке Java, предоставляющих интерфейсы
для решения многочисленных проблем NLP. В данной работе использовались две подсистемы CoreNLP:
POS-tagger, предназначенный для определения частей речи слов и их лемматизации,
и parser, предоставляющий возможность строить дерево зависимостей членов предложений.

