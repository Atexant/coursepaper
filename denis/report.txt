Википедия (Wikipedia) - свободно распространяемая и редактируемая интернет-энциклопедия, расположенная по адресу [1]. По сути она представляет собой набор взаимосвязанных статей, затрагивающих самые разные области человеческого знания. 

Интересной особенностью википедии является то, что если в содержании какой-то статьи упоминается понятие, которое напрямую связано с темой другой статьи, тогда в тексте первой статьи это понятие будет  выделено, как ссылка на вторую. Таким образом в отличие от обычных энциклопедий упрощается изучение того или иного аспекта засчет того, что встретив неизвестный термин или понятие, можно тут же перейти к статье о нем.

Кроме предоставления доступа через веб-интерфейс владельцы сайта дают возможность получить локальную копию текста всех статей Википедии [2], скачав дамп их содержимого.

Дамп базы данных Википедии[3] - это заархивированный XML-файл, размер которого в распакованном виде составляет порядка 36Гб. Как видно из XML-метасхемы[4] и при поверхностном изучении самого дампа, статьи в нем представлены в виде тэгов “page” верхнего уровня. Каждый из них содержит следующие вложенные теги:
a. title - содержит заголовок статьи, уникальный в рамках википедии
b. id - содержит уникальный идентификатор статьи, положительное целое число
с. revision - тег содержит описание последнего изменения статьи (дату, автора), а так же непосредственно последнюю версию текста статьи во вложенном тэге "text”. 
Тег “page”, соответсвующий какой-либо, может также содержать внутри себя элемент “redirect”, что означает, что статья с таким названием дублирует какую-то другую статью, в этом ее тег “text” содержит заголовок дублируемой статьи. 

*Нужны ли примеры содержимого тегов?*
Да, небольшие примеры вполне уместны.

Здесь нужен текст, который дал бы общие понятие вида,
что такие-то технологии у нас есть (т.е. DOM и SAX),
а при их использовании у нас есть ряд задач для исследования, из которых главная - время работы, эксперименты по обработке языка и т.д.
Про DOM/SAX я написал ниже, а насчет обработки языка я к сожалению не понял

На данный момент существует два основных метода программной обработки XML-файлов:
1. DOM(Document Object Model)[5] - удобный программный интерфейс доступа к XML-документу, рассматривающий его как дерево, где каждому элементу сопоставляется узел таким образом, что если элемент А вложен в элемент B в XML-документе, то в получившемся дереве узлы, соотвествующие этим элементам будут связаны отношением “родитель-потомок”.
Основным недостатком этого метода является то, что он подразумевает хранения полученного дерева в оперативной памяти, что в случае с файлами такого размера как дампы Wikipedia недопустимо.
2. SAX (Simple API for XML)[6] - метод последовательного чтения XML-документа. Его суть заключается в том, что обработчик последовательно обрабатывает данные из XML-файла, и встречая новые элементы разметки сообщает об этом вызвавшему приложению, используя клиентские функции обратного вызова[7]. Таким образом приложение-клиент SAX-парсера может получить полную информацию о содержимом и структуре документа, не загружая его целиком в память. Однако и у этого метода есть свои недостатки. Например, если нужно получить содержимое элемента, который находится в конце файла, нужно обработать все предшествующие элементы, что при размерах документа, сравнимых с размерами дампа википедии, может занять довольно много времени.

Учитывая достоинства SAX-парсеров, позволяющие обрабатывать объемные XML-документы, для реализации был выбран именно этот метод.

Вот здесь начинается описание подхода, но нужно, опять же, рассказать,
какие особенности учитывались при разработке этой архитектуры. Много не надо, но сейчас текст выглядит несвязным. 
Выше я добавил предложение-связку, или надо что-то более подробное?

На этапе разработки обработчика дампа Википедии еще не было хорошо известно, что делать с полученными данными дальше, поэтому было принято решение использовать следующую архитектуру:
1. Объект parser класса WikipediaParser запускает SAX-парсер, который “сообщает” ему о встреченных XML-элементах и их содержимом.
2. Предварительно этот объект конфигурируется другим объектом - обработчиком статей, реализующий интерфейс WikipediaPageHandler, содержащий метод handle(WikipediaPage).
3. parser при прочтении целиком тега page создает объект WikipediaPage, заполняет необходимые поля и передает его обработчику статей.

Таким образом была получена гибкая система обработки дампа, для использования которой достаточно реализовать простой интерфейс с обработкой одного объекта-страницы.

Если это было известно сразу, то надо в самом начале текста про это и упомянуть. 
Человек когда читает текст, сначала, получается, настраивает свои мысли на что-то одно, а потом при чтении абзаца ниже, начинает мысли перестраивать. Такого лучше избегать.
То есть нужно написать в начале документа, какие есть проблемы, а потом просто описывать их решения?

Практически сразу было замечено, что нужно как-то решать проблему доступа к произвольному участку документа, так как:
a. Это необходимо для параллельной обработки.
b. При исключительной ситуации, если обработчик некорректно завершает свою работу, необходимо было начинать обработку всего дампа сначала, что не очень удобно.

Для этого было решено создать обертку стандартного потока InputStream, которая:
1. Обладает возможностью задания сдвига в байтах от начала файла с XML-документом
2. Если заданный сдвиг не указывает на начало тега page, пропустить все байты вплоть до начала следующего такого тега
3. Искусственно “обернуть” отдаваемые потоком данные в корневой тег.
Последние два пункта нужны для того, чтобы содержимое получившегося потока соответсвовало формату XML

В результате была реализована возможность обрабатывать файл-дамп с произвольным сдвигом от его начала, что значительно увеличило удобство использования этой части системы.  

Текст статей Википедии включает в себя кроме всего прочего элементы разметки[8], которые позволяют:
1. Управлять внешним видом блоков текста (размер/тип шрифта, цвет, отступ и т.п.)
2. Создавать разного рода специфичные структуры оформления информации (таблицы, вложенные списки, оглавления)
3. Выделять некоторые части текста, как ссылки на другие статьи

Так как разметка статей содержит в себе важные, информационно-значимые части, необходимо было разработать систему, очищающую текст от ее частей, связанных с оформлением, а также предоставляющую удобный интерфейс доступа к информативным элементам.

В качестве решения этой проблемы было предложено использовать библиотеку JWPL (Java Wikipedia Library)[9], которая содержит инструменты по работе с разметкой Mediawiki[8] и дает программный доступ к отдельным ее элементам, таким как:
a. Список ссылок на другие статьи, встречающиеся в тексте этой статьи
b. Вложенные списки и оглавления, представленные в виде древообразных структур
c. Таблицы из конкретной статьи и удобный интерфейс для их обработки

*Вообще внутри Atexant я создал специальный класс доступа к этой библиотеки, чтобы  ей можно было удобно пользоваться, а недавно на занятиях по ООП узнал, что это называется паттерн “Шлюз”. Я вот не знаю, стоит ли рассказывать архитектурные подробности*

Литература
1. http://www.wikipedia.org/
2. http://en.wikipedia.org/wiki/Wikipedia:Database_download
3. http://download.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2
4. http://www.mediawiki.org/xml/export-0.5.xsd
5. http://ru.wikipedia.org/wiki/Document_Object_Model
6. http://ru.wikipedia.org/wiki/SAX
7. http://ru.wikipedia.org/wiki/Callback
8. http://en.wikipedia.org/wiki/Help:Wiki_markup
9. http://code.google.com/p/jwpl/
